{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "\n",
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data and viewing path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('/storage/data/mnist_png/training'),Path('/storage/data/mnist_png/testing')]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#6742) [Path('/storage/data/mnist_png/training/1/42690.png'),Path('/storage/data/mnist_png/training/1/49817.png'),Path('/storage/data/mnist_png/training/1/12078.png'),Path('/storage/data/mnist_png/training/1/5862.png'),Path('/storage/data/mnist_png/training/1/36368.png'),Path('/storage/data/mnist_png/training/1/57223.png'),Path('/storage/data/mnist_png/training/1/37725.png'),Path('/storage/data/mnist_png/training/1/54103.png'),Path('/storage/data/mnist_png/training/1/29986.png'),Path('/storage/data/mnist_png/training/1/18207.png')...]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/\"training/1\").ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function used to load data appropriatly\n",
    "def load_data(folder):\n",
    "    dataList = []\n",
    "    labelList = []\n",
    "    for num in range(10):\n",
    "        data_path = (path/folder/f'{num}').ls().sorted() #getting path\n",
    "        \n",
    "        stackedData = torch.stack([tensor(Image.open(o)) for o in data_path]) #Open each image and stack them\n",
    "        stackedData = stackedData.float()/255.0 #squishing between 0-1\n",
    "        \n",
    "        dataList.append(stackedData) #adding to dataList\n",
    "        labelList.extend([num]*len(data_path))#extending labelList\n",
    "    \n",
    "    #Convert into a single tensor with each image data in each row\n",
    "    train_x = torch.cat(dataList).view(-1, 28*28) \n",
    "    train_y = tensor(labelList)\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "train_x, train_y = load_data(\"training\")\n",
    "test_x, test_y = load_data(\"testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataloaders (Minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = list(zip(train_x,train_y))\n",
    "valid_dset = list(zip(test_x,test_y))\n",
    "\n",
    "dl_train = DataLoader(train_dset, batch_size=256)\n",
    "dl_test = DataLoader(valid_dset, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is what the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(train_x[0].view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the functions we need to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = F.cross_entropy(preds, yb)\n",
    "    loss.backward()\n",
    "\n",
    "def train_epoch(model):\n",
    "    for xb,yb in dl_train:\n",
    "        calc_grad(xb, yb, model)\n",
    "        \n",
    "        for p in params: \n",
    "            p.data -= p.grad.data * lr\n",
    "            p.grad.zero_()\n",
    "\n",
    "def batch_accuracy(xb, yb):\n",
    "    pred = xb.softmax(1) # gets final activations into range of 0,1 so that all add up to 1\n",
    "    accuracy = get_num_correct(pred, yb)/float(yb.size(0))\n",
    "    return accuracy\n",
    "\n",
    "def get_num_correct(preds, yb):\n",
    "    return preds.argmax(dim=1).eq(yb).sum().float()\n",
    "\n",
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in dl_test]\n",
    "    return round(torch.stack(accs).mean().item(), 4)\n",
    "\n",
    "def linear_layer(xb):\n",
    "    return xb@w + b\n",
    "\n",
    "def init_params(x, var=1.0): \n",
    "    return (torch.randn(x)*var).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin by initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "7aDM5v7oF3fO"
   },
   "outputs": [],
   "source": [
    "lr = 1.\n",
    "w = init_params((28*28,10))\n",
    "b = init_params(10)\n",
    "params = w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets see if our loss improves for 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.183"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_epoch(linear_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.289"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(linear_layer)\n",
    "validate_epoch(linear_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It improved, so now lets train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2726 0.308 0.3438 0.3682 0.3947 0.4164 0.4332 0.4462 0.4539 0.4604 0.466 0.4692 0.4732 0.4762 0.4793 0.4825 0.4843 0.4858 0.4879 0.4896 "
     ]
    }
   ],
   "source": [
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model), end=' ')\n",
    "        \n",
    "train_model(linear_layer, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can do all of the above very easily by using FastAI toolkit\n",
    "### Additionally, I will also add some nonlinearity this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.460385</td>\n",
       "      <td>3.105183</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.930706</td>\n",
       "      <td>2.921132</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.712777</td>\n",
       "      <td>2.385295</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.599163</td>\n",
       "      <td>2.020884</td>\n",
       "      <td>0.324600</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.531909</td>\n",
       "      <td>1.787350</td>\n",
       "      <td>0.385900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.485915</td>\n",
       "      <td>1.628456</td>\n",
       "      <td>0.423500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.451067</td>\n",
       "      <td>1.514974</td>\n",
       "      <td>0.450500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.423412</td>\n",
       "      <td>1.429631</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.400912</td>\n",
       "      <td>1.362436</td>\n",
       "      <td>0.490200</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.382621</td>\n",
       "      <td>1.307440</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.367557</td>\n",
       "      <td>1.261222</td>\n",
       "      <td>0.519900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.355110</td>\n",
       "      <td>1.221475</td>\n",
       "      <td>0.534200</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.344696</td>\n",
       "      <td>1.186626</td>\n",
       "      <td>0.548400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.335849</td>\n",
       "      <td>1.156045</td>\n",
       "      <td>0.562100</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.328118</td>\n",
       "      <td>1.129085</td>\n",
       "      <td>0.574200</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.321477</td>\n",
       "      <td>1.104802</td>\n",
       "      <td>0.585800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>1.083139</td>\n",
       "      <td>0.597400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.310001</td>\n",
       "      <td>1.063582</td>\n",
       "      <td>0.604800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.304906</td>\n",
       "      <td>1.045692</td>\n",
       "      <td>0.611600</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.300314</td>\n",
       "      <td>1.028769</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = DataLoaders(dl_train, dl_test)\n",
    "\n",
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,30), #30 neurons\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 10) # 30neurons into 10 output neurons (10 classes)\n",
    ")\n",
    "\n",
    "learn = Learner(dls, simple_net, opt_func=SGD,\n",
    "                loss_func=F.cross_entropy, metrics=accuracy)\n",
    "\n",
    "learn.fit(20, .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So it seems that adding nonlinearity increased the accuracy by 10%!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
