{
  
    
        "post0": {
            "title": "Tabular on Lesson 8",
            "content": "Tabular . The collaborative filtering problem we will be doing is one we have done before in lesson 8. But, what if try doing it using a tabular model. Note: We learned tabular modeling in lesson 9! . from fastai.collab import * from fastai.tabular.all import * from sklearn.ensemble import RandomForestRegressor . Data . Lets grab the data, like before, and view it . path = untar_data(URLs.ML_100k) ratings = pd.read_csv(path/&#39;u.data&#39;, delimiter=&#39; t&#39;, header=None, names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) ratings.head() . user movie rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . Grabbing data . Let&#39;s initialize some variables we need to create our tabular object. . user_name = ratings.columns[0] #user col item_name = ratings.columns[1] #movie col rating_name = ratings.columns[2] #label (rating col) cat_names = [user_name,item_name] #category col splits = RandomSplitter()(range_of(ratings)) #split procs = [Categorify, FillMissing, Normalize] . Tabular Object . Lets now take everything above and create a tabular object. . to = TabularCollab(ratings, procs, cat_names, y_names=[rating_name], y_block=TransformBlock(), splits=splits, reduce_memory=False) dls = to.dataloaders() . dls.show_batch() . user movie rating . 0 614 | 476 | 3 | . 1 582 | 676 | 2 | . 2 792 | 124 | 4 | . 3 733 | 762 | 4 | . 4 344 | 815 | 2 | . 5 36 | 269 | 3 | . 6 110 | 779 | 3 | . 7 752 | 302 | 5 | . 8 409 | 214 | 4 | . 9 916 | 50 | 5 | . Training . Now let&#39;s find the best lr and train our model. . learn = tabular_learner(dls, y_range=(0,5.5), layers=[500,250], n_out=1, loss_func=MSELossFlat()) learn.lr_find() #find best lr . SuggestedLRs(lr_min=0.00831763744354248, lr_steep=0.0008317637839354575) . learn.fit_one_cycle(4, 1e-3, wd=.01) . epoch train_loss valid_loss time . 0 | 0.973944 | 0.977132 | 00:12 | . 1 | 0.875915 | 0.895781 | 00:12 | . 2 | 0.807531 | 0.848947 | 00:12 | . 3 | 0.727740 | 0.856915 | 00:12 | . If you compare these results with lesson 8, you will notice they are similer. . Random Forest . We can also create a random forest, which we also learned in lesson 9! . def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6) def m_rmse(m, xs, y): return r_mse(m.predict(xs), y) . Method below creates our random forest and fits it . def rf(xs, y, n_estimators=40, max_samples=80000, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) . Tabular object . As we did above, lets create our tabular object. . procs = [Categorify, FillMissing] to = TabularCollab(ratings, procs, cat_names, y_names=[rating_name], y_block=TransformBlock(), splits=splits, reduce_memory=False) . xs,y = to.train.xs, to.train.y valid_xs,valid_y = to.valid.xs, to.valid.y . m = rf(xs, y) #Fitting . m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) . (0.848744, 1.004573) .",
            "url": "https://usama280.github.io/PasteBlogs/2021/08/22/Tabular-on-Lesson8.html",
            "relUrl": "/2021/08/22/Tabular-on-Lesson8.html",
            "date": " • Aug 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "MNIST Classifier Model",
            "content": "Goal . Now that we have created a model that can classify 3&#39;s and 7&#39;2, lets create a model for the entire MNIST dataset with all the numbers 0-9. . Getting data and viewing path . path = untar_data(URLs.MNIST) path.ls() . (#2) [Path(&#39;/storage/data/mnist_png/training&#39;),Path(&#39;/storage/data/mnist_png/testing&#39;)] . (path/&quot;training/1&quot;).ls() . (#6742) [Path(&#39;/storage/data/mnist_png/training/1/42690.png&#39;),Path(&#39;/storage/data/mnist_png/training/1/49817.png&#39;),Path(&#39;/storage/data/mnist_png/training/1/12078.png&#39;),Path(&#39;/storage/data/mnist_png/training/1/5862.png&#39;),Path(&#39;/storage/data/mnist_png/training/1/36368.png&#39;),Path(&#39;/storage/data/mnist_png/training/1/57223.png&#39;),Path(&#39;/storage/data/mnist_png/training/1/37725.png&#39;),Path(&#39;/storage/data/mnist_png/training/1/54103.png&#39;),Path(&#39;/storage/data/mnist_png/training/1/29986.png&#39;),Path(&#39;/storage/data/mnist_png/training/1/18207.png&#39;)...] . This is what the data looks like . t = (path/&quot;training/1&quot;).ls() t_1 = Image.open(t[0]) t_1 . show_image(tensor(t_1)) . &lt;AxesSubplot:&gt; . Loading data . To load our data, it may be beneficial to create a method to handle this process . def load_data(folder): dataList = [] labelList = [] for num in range(10): data_path = (path/folder/f&#39;{num}&#39;).ls().sorted() #getting path stackedData = torch.stack([tensor(Image.open(o)) for o in data_path]) #Open each image and stack them stackedData = stackedData.float()/255.0 #squishing between 0-1 dataList.append(stackedData) #adding to dataList labelList.extend([num]*len(data_path))#extending labelList #Convert so that each image data is in each row train_x = torch.cat(dataList).view(-1, 28*28) train_y = tensor(labelList) return train_x, train_y train_x, train_y = load_data(&quot;training&quot;) test_x, test_y = load_data(&quot;testing&quot;) . Creating dataloaders (Minibatches) . train_dset = list(zip(train_x,train_y)) valid_dset = list(zip(test_x,test_y)) dl_train = DataLoader(train_dset, batch_size=256) dl_test = DataLoader(valid_dset, batch_size=256) . Below is the functions we need to train and test the model . Most of these functions are copies and pasted from our previous MNIST model. The difference here is the loss function, which was swapped out for cross entropy (As we have multiple categories). And, our accuracy function has been adjusted due to switching out sigmoid for softmax (softmax ranges all values between 0-1). . def calc_grad(xb, yb, model): preds = model(xb) loss = F.cross_entropy(preds, yb) loss.backward() def train_epoch(model): for xb,yb in dl_train: calc_grad(xb, yb, model) for p in params: p.data -= p.grad.data * lr p.grad.zero_() def batch_accuracy(xb, yb): pred = xb.softmax(1) return batch_accuracy_helper(pred, yb)/float(yb.size(0)) def batch_accuracy_helper(preds, yb): return preds.argmax(dim=1).eq(yb).sum().float() def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in dl_test] return round(torch.stack(accs).mean().item(), 4) def linear_layer(xb): return xb@w + b def init_params(x, var=1.0): return (torch.randn(x)*var).requires_grad_() . Begin by initializing parameters . lr = 1. w = init_params((28*28,10)) b = init_params(10) params = w, b w.shape, b.shape . (torch.Size([784, 10]), torch.Size([10])) . Now lets see if our loss improves for 1 epoch . It&#39;s good practice to try training the model manually to see if the loss improves. If it doesn&#39;t this means there may be some error. . validate_epoch(linear_layer) . 0.1534 . train_epoch(linear_layer) validate_epoch(linear_layer) . 0.1814 . Our loss improved, nice! . Training . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) train_model(linear_layer, 20) . 0.255 0.3079 0.3501 0.3783 0.4011 0.4184 0.4319 0.442 0.4507 0.4583 0.4647 0.4711 0.4758 0.4803 0.4833 0.4862 0.488 0.4887 0.4906 0.4925 . 50% acc is not that bad, given there are 10 classes . Using FastAI toolkit . As before, we can take everything we did above and condense it using FastAI&#39;s toolkit. Additionally, I will add non-linearity this time to see how much of a performance boost it gives. . dls = DataLoaders(dl_train, dl_test) simple_net = nn.Sequential( nn.Linear(28*28,30), #30 neurons nn.ReLU(), nn.Linear(30, 10) # 30neurons into 10 output neurons (10 classes) ) . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=F.cross_entropy, metrics=accuracy) learn.fit(20, .01) . epoch train_loss valid_loss accuracy time . 0 | 1.524066 | 2.803759 | 0.100900 | 00:01 | . 1 | 0.953391 | 2.718991 | 0.102200 | 00:01 | . 2 | 0.708434 | 2.372834 | 0.182800 | 00:01 | . 3 | 0.585201 | 2.130398 | 0.294700 | 00:01 | . 4 | 0.512490 | 1.966653 | 0.355500 | 00:01 | . 5 | 0.463571 | 1.847218 | 0.389500 | 00:01 | . 6 | 0.428147 | 1.752266 | 0.416000 | 00:01 | . 7 | 0.401295 | 1.671586 | 0.435400 | 00:01 | . 8 | 0.380187 | 1.600696 | 0.452600 | 00:01 | . 9 | 0.363209 | 1.537549 | 0.467200 | 00:01 | . 10 | 0.349405 | 1.479971 | 0.482300 | 00:01 | . 11 | 0.338045 | 1.427443 | 0.496500 | 00:01 | . 12 | 0.328442 | 1.379472 | 0.509500 | 00:01 | . 13 | 0.320192 | 1.335956 | 0.522400 | 00:01 | . 14 | 0.313182 | 1.295694 | 0.534100 | 00:01 | . 15 | 0.307200 | 1.257923 | 0.546100 | 00:01 | . 16 | 0.302036 | 1.222951 | 0.556100 | 00:01 | . 17 | 0.297584 | 1.189870 | 0.567500 | 00:01 | . 18 | 0.293659 | 1.158959 | 0.576600 | 00:01 | . 19 | 0.290083 | 1.129629 | 0.586000 | 00:01 | . So it seems that adding nonlinearity increased the accuracy by 10! . Now lets try refining the learning rate . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=F.cross_entropy, metrics=accuracy) learn.fine_tune(2, base_lr=0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.428609 | 6.502268 | 0.100900 | 00:01 | . epoch train_loss valid_loss accuracy time . 0 | 0.311028 | 4.431973 | 0.100900 | 00:01 | . 1 | 1.791578 | 1.622997 | 0.327900 | 00:01 | . lr_min, lr_steep = learn.lr_find() #Finding best print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . Minimum/10: 3.31e-01, steepest point: 3.31e-02 . learn.fine_tune(20, base_lr=3e-2) #Now lets train on the steepest . epoch train_loss valid_loss accuracy time . 0 | 0.302600 | 3.178728 | 0.153700 | 00:02 | . epoch train_loss valid_loss accuracy time . 0 | 0.833551 | 1.429206 | 0.407900 | 00:01 | . 1 | 0.618095 | 1.583635 | 0.398200 | 00:01 | . 2 | 0.459298 | 1.740223 | 0.393500 | 00:01 | . 3 | 0.361341 | 1.833040 | 0.392500 | 00:01 | . 4 | 0.308362 | 1.860949 | 0.398500 | 00:01 | . 5 | 0.284963 | 1.830225 | 0.410000 | 00:01 | . 6 | 0.278647 | 1.763200 | 0.424700 | 00:01 | . 7 | 0.277990 | 1.687577 | 0.440100 | 00:01 | . 8 | 0.282089 | 1.601443 | 0.458500 | 00:01 | . 9 | 0.291083 | 1.502006 | 0.477600 | 00:01 | . 10 | 0.305329 | 1.387264 | 0.501700 | 00:01 | . 11 | 0.325475 | 1.255826 | 0.530700 | 00:01 | . 12 | 0.352534 | 1.109034 | 0.572600 | 00:01 | . 13 | 0.388066 | 0.950091 | 0.625700 | 00:01 | . 14 | 0.433138 | 0.787106 | 0.689900 | 00:01 | . 15 | 0.487192 | 0.635287 | 0.761700 | 00:01 | . 16 | 0.542165 | 0.518766 | 0.828100 | 00:01 | . 17 | 0.574565 | 0.459373 | 0.863400 | 00:01 | . 18 | 0.569960 | 0.444706 | 0.870800 | 00:01 | . 19 | 0.554268 | 0.443647 | 0.871400 | 00:02 | . Adjusting the LR improved acc by 25! .",
            "url": "https://usama280.github.io/PasteBlogs/2021/08/22/MNIST.html",
            "relUrl": "/2021/08/22/MNIST.html",
            "date": " • Aug 22, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Lesson 12 - FastAI",
            "content": "A Language Model from Scratch . We have worked with NLP models in the previous lecture, and have seen the many benefits and capabilities of such a model. Now let&#39;s try to create our very own model from scratch! . The Data . from fastai.text.all import * path = untar_data(URLs.HUMAN_NUMBERS) . path.ls() . (#2) [Path(&#39;valid.txt&#39;),Path(&#39;train.txt&#39;)] . lines = L() with open(path/&#39;train.txt&#39;) as f: lines += L(*f.readlines()) with open(path/&#39;valid.txt&#39;) as f: lines += L(*f.readlines()) lines . (#9998) [&#39;one n&#39;,&#39;two n&#39;,&#39;three n&#39;,&#39;four n&#39;,&#39;five n&#39;,&#39;six n&#39;,&#39;seven n&#39;,&#39;eight n&#39;,&#39;nine n&#39;,&#39;ten n&#39;...] . This is what our data looks like right now . text = &#39; . &#39;.join([l.strip() for l in lines]) #Reformating text[:100] . &#39;one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo&#39; . tokens = text.split(&#39; &#39;) #Now lets tokenize it tokens[:10] . [&#39;one&#39;, &#39;.&#39;, &#39;two&#39;, &#39;.&#39;, &#39;three&#39;, &#39;.&#39;, &#39;four&#39;, &#39;.&#39;, &#39;five&#39;, &#39;.&#39;] . vocab = L(*tokens).unique() #lets create our vocab vocab . (#30) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;,&#39;five&#39;,&#39;six&#39;,&#39;seven&#39;,&#39;eight&#39;,&#39;nine&#39;...] . This will be our vocab:Lets also numericalize it. . word2idx = {w:i for i,w in enumerate(vocab)} #Dictionary of word:id nums = L(word2idx[i] for i in tokens) #Numericalization tokens[:10], nums . ([&#39;one&#39;, &#39;.&#39;, &#39;two&#39;, &#39;.&#39;, &#39;three&#39;, &#39;.&#39;, &#39;four&#39;, &#39;.&#39;, &#39;five&#39;, &#39;.&#39;], (#63095) [0,1,2,1,3,1,4,1,5,1...]) . Dataloader . L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3)) . (#21031) [([&#39;one&#39;, &#39;.&#39;, &#39;two&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;three&#39;, &#39;.&#39;], &#39;four&#39;),([&#39;four&#39;, &#39;.&#39;, &#39;five&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;six&#39;, &#39;.&#39;], &#39;seven&#39;),([&#39;seven&#39;, &#39;.&#39;, &#39;eight&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;nine&#39;, &#39;.&#39;], &#39;ten&#39;),([&#39;ten&#39;, &#39;.&#39;, &#39;eleven&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;twelve&#39;, &#39;.&#39;], &#39;thirteen&#39;),([&#39;thirteen&#39;, &#39;.&#39;, &#39;fourteen&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;fifteen&#39;, &#39;.&#39;], &#39;sixteen&#39;)...] . Tokens are created so that the 4th token is the label. . seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3)) #Lets do the above, but this time using seqs #numericalization form . (#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10, 1, 11]), 1),(tensor([ 1, 12, 1]), 13),(tensor([13, 1, 14]), 1),(tensor([ 1, 15, 1]), 16)...] . bs = 64 cut = int(len(seqs) * 0.8) #80% training set, 20% valid set dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False) . dls.one_batch()[0][:2] . tensor([[0, 1, 2], [1, 3, 1]]) . Our Language Model in PyTorch . Below is our language model. As you see, we have created three layers: . The embedding layer (i_h, for input to hidden) | The linear layer to create the activations for the next word (h_h, for hidden to hidden) | A final linear layer to predict the fourth word (h_o, for hidden to output) | . class LMModel1(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden)#input self.h_h = nn.Linear(n_hidden, n_hidden)#hidden self.h_o = nn.Linear(n_hidden,vocab_sz)#output def forward(self, x): h = F.relu(self.h_h(self.i_h(x[:,0]))) #word 1 h = h + self.i_h(x[:,1]) #word 2 h = F.relu(self.h_h(h)) h = h + self.i_h(x[:,2]) #word 3 h = F.relu(self.h_h(h)) return self.h_o(h) #pred (word 4) . Train . learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.794209 | 2.036811 | 0.466128 | 00:03 | . 1 | 1.384254 | 1.801755 | 0.473734 | 00:03 | . 2 | 1.404778 | 1.655324 | 0.494176 | 00:04 | . 3 | 1.369884 | 1.709227 | 0.423104 | 00:03 | . Awesome we created our first NLP model! . n,counts = 0,torch.zeros(len(vocab)) for x,y in dls.valid: n += y.shape[0] for i in range_of(vocab): counts[i] += (y==i).long().sum() idx = torch.argmax(counts) idx, vocab[idx.item()], counts[idx].item()/n . (tensor(29), &#39;thousand&#39;, 0.15165200855716662) . Our acc would have been .15 had we used an naive model . Refining our model - Recurrent Neural Network . Lets refactor our above model. . class LMModel2(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) def forward(self, x): h = 0 for i in range(3): #lets use a forloop to create layers h = h + self.i_h(x[:,i]) h = F.relu(self.h_h(h)) return self.h_o(h) . Notice that here h is set to 0 after every batch . learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.876980 | 2.084122 | 0.410744 | 00:03 | . 1 | 1.407598 | 1.821299 | 0.467316 | 00:03 | . 2 | 1.410389 | 1.680269 | 0.490373 | 00:03 | . 3 | 1.372142 | 1.709884 | 0.415498 | 00:03 | . Roughly the same as we expected. However, what we have created this time is actually an RNN! . Improving the RNN . Maintaining the State of an RNN . The first way we can improve our model is by actually remembering the state of h from the previous batches. Recall that before we set h=0 after every batch. . class LMModel3(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) self.h = 0 def forward(self, x): for i in range(3): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) out = self.h_o(self.h) self.h = self.h.detach() #detach throws away the stored gradients - However, the activations are still stored return out #Start of each epoch, we should reset out h def reset(self): self.h = 0 . m = len(seqs)//bs m,bs,len(seqs) . (328, 64, 21031) . Minibatches . Recall from Lecture 10 how we created the minibatches. Where the nth row from a minibatch followed the nth row from the previous minibatch. The function below does exactly that for us. . def group_chunks(ds, bs): m = len(ds) // bs new_ds = L() for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs)) return new_ds . group_chunks(seqs, bs)[1:4] . (#3) [(tensor([3, 1, 2]), 28),(tensor([28, 24, 2]), 1),(tensor([ 1, 6, 28]), 25)] . seqs[m,m*2,m*3] . (#3) [(tensor([3, 1, 2]), 28),(tensor([28, 24, 2]), 1),(tensor([ 1, 6, 28]), 25)] . Lets recreate our dataloader using our improved minibatch format . cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets( group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) . learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy, cbs=ModelResetter) #This will call our reset function learn.fit_one_cycle(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.706451 | 1.823746 | 0.443510 | 00:04 | . 1 | 1.282585 | 1.720615 | 0.455048 | 00:03 | . 2 | 1.100162 | 1.534231 | 0.531250 | 00:03 | . 3 | 1.031388 | 1.547766 | 0.532933 | 00:03 | . 4 | 0.971291 | 1.532978 | 0.558654 | 00:03 | . 5 | 0.929672 | 1.446295 | 0.571154 | 00:03 | . 6 | 0.883135 | 1.520370 | 0.588221 | 00:03 | . 7 | 0.824741 | 1.607137 | 0.599038 | 00:03 | . 8 | 0.789257 | 1.675977 | 0.594952 | 00:03 | . 9 | 0.776834 | 1.629597 | 0.596875 | 00:03 | . Creating More Signal . Rather than predicting every 4th word, why don&#39;t we predict every other word. . sl = 16 seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1])) for i in range(0,len(nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) #dataloader dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) . seqs[0] . (tensor([0, 1, 2, 1, 3, 1, 4, 1, 5, 1, 6, 1, 7, 1, 8, 1]), tensor([1, 2, 1, 3, 1, 4, 1, 5, 1, 6, 1, 7, 1, 8, 1, 9])) . [vocab[s] for s in seqs[0]] . [(#16) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;...], (#16) [&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;,&#39;six&#39;...]] . class LMModel4(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) self.h = 0 def forward(self, x): outs = [] #list of output for i in range(sl): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) outs.append(self.h_o(self.h)) #append self.h = self.h.detach() return torch.stack(outs, dim=1) #stack of outputs def reset(self): self.h = 0 . def loss_func(inp, targ): return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1)) . learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.208388 | 2.989674 | 0.220052 | 00:01 | . 1 | 2.304600 | 1.926858 | 0.457845 | 00:01 | . 2 | 1.737188 | 1.785296 | 0.450684 | 00:01 | . 3 | 1.462938 | 1.722541 | 0.492106 | 00:01 | . 4 | 1.269122 | 1.607646 | 0.568197 | 00:01 | . 5 | 1.122454 | 1.725385 | 0.579508 | 00:01 | . 6 | 0.989286 | 1.876261 | 0.620443 | 00:01 | . 7 | 0.877782 | 2.080590 | 0.626383 | 00:01 | . 8 | 0.779877 | 2.068581 | 0.646729 | 00:01 | . 9 | 0.702537 | 2.105229 | 0.655518 | 00:01 | . 10 | 0.648459 | 2.225554 | 0.670654 | 00:01 | . 11 | 0.602616 | 2.259415 | 0.672607 | 00:01 | . 12 | 0.571914 | 2.272124 | 0.676270 | 00:01 | . 13 | 0.552240 | 2.258376 | 0.678874 | 00:01 | . 14 | 0.540891 | 2.214495 | 0.678630 | 00:01 | . Better than before! . Multilayer RNNs . Lets create a deeper and more layered RNN. What&#39;s unique about this RNN is that each layer will have a different weight matrix. Lets change our model and see how it performs. . The Model . class LMModel5(Module): def __init__(self, vocab_sz, n_hidden, n_layers): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = torch.zeros(n_layers, bs, n_hidden) #How many layer to stack self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True) #This is doing what our previous model did: #Looping of the layers def forward(self, x): res,h = self.rnn(self.i_h(x), self.h) #Notice that we can do the loop by calling our RNN self.h = h.detach() return self.h_o(res) def reset(self): self.h.zero_() . learn = Learner(dls, LMModel5(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.014767 | 2.582862 | 0.420003 | 00:02 | . 1 | 2.149063 | 1.779345 | 0.471354 | 00:02 | . 2 | 1.704159 | 1.854296 | 0.351156 | 00:02 | . 3 | 1.472523 | 1.680113 | 0.467692 | 00:02 | . 4 | 1.299899 | 1.845994 | 0.488200 | 00:02 | . 5 | 1.145692 | 2.308071 | 0.487874 | 00:02 | . 6 | 1.022578 | 2.543387 | 0.480794 | 00:01 | . 7 | 0.923336 | 2.659213 | 0.493815 | 00:02 | . 8 | 0.822356 | 2.721887 | 0.509277 | 00:02 | . 9 | 0.733957 | 2.826130 | 0.524740 | 00:02 | . 10 | 0.663029 | 2.933543 | 0.532878 | 00:02 | . 11 | 0.612702 | 2.961933 | 0.537842 | 00:02 | . 12 | 0.577110 | 3.006170 | 0.538493 | 00:02 | . 13 | 0.555790 | 3.018762 | 0.536133 | 00:02 | . 14 | 0.544564 | 3.017484 | 0.538574 | 00:02 | . Our model did worse. Does that mean our model is bad? No, what most likley happened here is that our gradient has either exploded or disappeared. . LSTM . We can fix the issue of gradients exploding or disappearing by creating another type of architecture, LSTM. . Building an LSTM from Scratch . class LSTMCell(Module): def __init__(self, ni, nh): self.forget_gate = nn.Linear(ni + nh, nh) self.input_gate = nn.Linear(ni + nh, nh) self.cell_gate = nn.Linear(ni + nh, nh) self.output_gate = nn.Linear(ni + nh, nh) def forward(self, input, state): h,c = state h = torch.cat([h, input], dim=1) forget = torch.sigmoid(self.forget_gate(h)) c = c * forget inp = torch.sigmoid(self.input_gate(h)) cell = torch.tanh(self.cell_gate(h)) c = c + inp * cell out = torch.sigmoid(self.output_gate(h)) h = out * torch.tanh(c) return h, (h,c) . We can refactor the above code . class LSTMCell(Module): def __init__(self, ni, nh): self.ih = nn.Linear(ni,4*nh) self.hh = nn.Linear(nh,4*nh) def forward(self, input, state): h,c = state # One big multiplication for all the gates is better than 4 smaller ones gates = (self.ih(input) + self.hh(h)).chunk(4, 1) ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3]) cellgate = gates[3].tanh() c = (forgetgate*c) + (ingate*cellgate) h = outgate * c.tanh() return h, (h,c) . t = torch.arange(0,10); t . tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . t.chunk(2) . (tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9])) . Training a Language Model Using LSTMs . class LMModel6(Module): def __init__(self, vocab_sz, n_hidden, n_layers): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)] #more hidden state layers because LSTM has more layers self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) #Replace our RNN to LSTM def forward(self, x): res,h = self.rnn(self.i_h(x), self.h) self.h = [h_.detach() for h_ in h] return self.h_o(res) def reset(self): for h in self.h: h.zero_() . learn = Learner(dls, LMModel6(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 3.012262 | 2.700189 | 0.295003 | 00:02 | . 1 | 2.168225 | 1.905885 | 0.366048 | 00:03 | . 2 | 1.620007 | 1.739222 | 0.475830 | 00:03 | . 3 | 1.364854 | 2.005928 | 0.523926 | 00:02 | . 4 | 1.133465 | 2.469636 | 0.541504 | 00:03 | . 5 | 0.916891 | 2.274207 | 0.563883 | 00:03 | . 6 | 0.715401 | 2.295597 | 0.635661 | 00:03 | . 7 | 0.535663 | 2.418355 | 0.629964 | 00:03 | . 8 | 0.387026 | 2.185029 | 0.680094 | 00:03 | . 9 | 0.284487 | 2.342169 | 0.701497 | 00:03 | . 10 | 0.212791 | 2.192696 | 0.718994 | 00:03 | . 11 | 0.153409 | 2.317826 | 0.720540 | 00:03 | . 12 | 0.115390 | 2.283189 | 0.730957 | 00:03 | . 13 | 0.092992 | 2.291156 | 0.729574 | 00:03 | . 14 | 0.082400 | 2.273516 | 0.731771 | 00:03 | . Our model is doing much better now . Regularizing an LSTM . Will be using some regularization technique to improve our model&#39;s gradients, particularly Dropout. What dropout does is that it randomly drops some neurons each minibatch: This forces the model to become more robust by making it able to produce the correct prediction even with less neurons available. . Dropout . class Dropout(Module): def __init__(self, p): self.p = p #probility that activation gets deleted def forward(self, x): if not self.training: #NO DROPOUT DURING TESTING (Only occurs during training) return x mask = x.new(*x.shape).bernoulli_(1-self.p) #1&#39;s and 0&#39;s where 1-p is the prob that we get a 1 return x * mask.div_(1-self.p) . Activation Regularization and Temporal Activation Regularization . Activation regularization (AR) and temporal activation regularization (TAR) are two regularization methods very similar to weight decay, which we have discussed before. . For activation regularization, it&#39;s the final activations produced by the LSTM that we will try to make as small as possible, instead of the weights. . loss += alpha * activations.pow(2).mean() . Temporal activation regularization is there to encourage that behavior by adding a penalty to the loss to make the difference between two consecutive activations as small as possible: . loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean() . Training a Weight-Tied Regularized LSTM . class LMModel7(Module): def __init__(self, vocab_sz, n_hidden, n_layers, p): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) self.drop = nn.Dropout(p) #Dropout self.h_o = nn.Linear(n_hidden, vocab_sz) self.h_o.weight = self.i_h.weight #Hidden-to-output weights are set identical to input-to-hidden self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)] def forward(self, x): raw,h = self.rnn(self.i_h(x), self.h) out = self.drop(raw) self.h = [h_.detach() for h_ in h] return self.h_o(out),raw,out def reset(self): for h in self.h: h.zero_() . Notice that Hidden-to-output and input-to-hidden are linked by the same parameters . learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)]) #Although we didn&#39;t create our regularizer, we can still #pass it via cbs. . learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . Calling TextLearner will automatically add ModelResetter, RNNRegularizer(alpha=2, beta=1) for us . learn.fit_one_cycle(15, 1e-2, wd=0.1) . epoch train_loss valid_loss accuracy time . 0 | 2.797331 | 2.223156 | 0.435465 | 00:03 | . 1 | 1.981550 | 1.755357 | 0.458740 | 00:03 | . 2 | 1.272993 | 0.754981 | 0.765381 | 00:03 | . 3 | 0.728812 | 0.600991 | 0.828125 | 00:03 | . 4 | 0.439376 | 0.546993 | 0.836995 | 00:03 | . 5 | 0.298545 | 0.453179 | 0.866781 | 00:03 | . 6 | 0.224571 | 0.446198 | 0.865641 | 00:03 | . 7 | 0.184994 | 0.472140 | 0.862793 | 00:03 | . 8 | 0.159867 | 0.493649 | 0.847900 | 00:03 | . 9 | 0.143191 | 0.476974 | 0.852458 | 00:03 | . 10 | 0.131329 | 0.475887 | 0.851318 | 00:03 | . 11 | 0.122343 | 0.522000 | 0.833333 | 00:02 | . 12 | 0.115645 | 0.531508 | 0.827881 | 00:03 | . 13 | 0.111532 | 0.502286 | 0.835856 | 00:04 | . 14 | 0.108859 | 0.507470 | 0.835286 | 00:03 | . Conclusion . Overall, we were not only able to create a NLP model from scratch but also refine it using LSTM&#39;s and Dropout. . Questionnaire . If the dataset for your project is so big and complicated that working with it takes a significant amount of time, what should you do? Create a simple dataset that allow for quick and easy prototyping. | Why do we concatenate the documents in our dataset before creating a language model? This allows us to easily split up data into batches. | To use a standard fully connected network to predict the fourth word given the previous three words, what two tweaks do we need to make to our model? Use same weight matrix for the three layers. Use the first word’s embeddings as activations to pass to linear layer, add the second word’s embeddings to the first layer’s output activations, and continues for rest of words. | How can we share a weight matrix across multiple layers in PyTorch? Define one layer in the PyTorch model class and use it multiple times in the forward pass. | Write a module that predicts the third word given the previous two words of a sentence, without peeking. . class LMModel1(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) def forward(self, x): h = 0 for i in range(3): h = h + self.i_h(x[:,i]) h = F.relu(self.h_h(h)) return self.h_o(h) . | What is a recurrent neural network? A refactoring of a multi-layer neural network as a loop. | What is &quot;hidden state&quot;? Hidden state&#39;s are the activations updated after each RNN step. | What is the equivalent of hidden state in LMModel1? h | To maintain the state in an RNN, why is it important to pass the text to the model in order? Because state is maintained over all batches, so order matters. | What is an &quot;unrolled&quot; representation of an RNN? A representation without loops. | Why can maintaining the hidden state in an RNN lead to memory and performance problems? How do we fix this problem? Backpropagation would cause it to calculate the gradients of all the past calls. This can be avoided using detach(). | What is &quot;BPTT&quot;? Calculating backpropagation only for the given batch (detach()). | Write code to print out the first few batches of the validation set, including converting the token IDs back into English strings, as we showed for batches of IMDb data in &lt;&gt;.&lt;/strong&gt; [vocab[s] for s in dls.one_batch[0]] . &lt;/li&gt; What does the ModelResetter callback do? Why do we need it? It calls our reset method, which resets our hidden state before every epoch. | What are the downsides of predicting just one output word for each three input words? There is a lot of extra information for training the model that is not being used. | Why do we need a custom loss function for LMModel4? We have a stacked output, which we need to flatten as CrossEntropyLoss expects flattened tensors. | Why is the training of LMModel4 unstable? Because this network is very deep it leads gradient to explode or disappear | In the unrolled representation, we can see that a recurrent neural network actually has many layers. So why do we need to stack RNNs to get better results? Because only one weight matrix is really being used. We can fix this by stacking. | Draw a representation of a stacked (multilayer) RNN. | Why should we get better results in an RNN if we call detach less often? Why might this not happen in practice with a simple RNN? | Why can a deep network result in very large or very small activations? Why does this matter? Numbers that are slightly large or small can lead to the explosion or disappearance of the number after repeated multiplications. In deep networks, we have repeated matrix multiplications, so this is a big problem. | In a computer&#39;s floating-point representation of numbers, which numbers are the most precise? Small numbers (Not too close to 0 however) | Why do vanishing gradients prevent training? No gradients mean no change in weights | Why does it help to have two hidden states in the LSTM architecture? What is the purpose of each one? One state remembers what happened earlier in the sentence, and the other predicts the next token. | What are these two states called in an LSTM? Cell state (long short-term memory) Hidden state (prediction) | What is tanh, and how is it related to sigmoid? A sigmoid function rescaled to the range of -1 to 1 | What is the purpose of this code in LSTMCell: h = torch.cat([h, input], dim=1) Joins the hidden state and the new input. | What does chunk do in PyTorch? Splits tensor in equal sizes. | Study the refactored version of LSTMCell carefully to ensure you understand how and why it does the same thing as the non-refactored version. | Why can we use a higher learning rate for LMModel6? Because now that we are using an LSTM, we have a partial solution to exploding/vanishing gradients. | What are the three regularization techniques used in an AWD-LSTM model? Dropout Activation regularization Temporal activation regularization | What is &quot;dropout&quot;? Random removal of neurons | Why do we scale the weights with dropout? Is this applied during training, inference, or both? The scale changes if we sum up activations, so to correct the scale, a division by (1-p) is applied. We applied this only during training, but can be done both ways. | What is the purpose of this line from Dropout: if not self.training: return x Prevents the usage of dropout during testing. | Experiment with bernoulli_ to understand how it works. | How do you set your model in training mode in PyTorch? In evaluation mode? Module.train(), Module.eval() | Write the equation for activation regularization (in math or code, as you prefer). How is it different from weight decay? loss += alpha * activations.pow(2).mean() . It&#39;s different because here we are not decreasing the weights, rather the activations | Write the equation for temporal activation regularization (in math or code, as you prefer). Why wouldn&#39;t we use this for computer vision problems? loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean() . This focuses on making the activations of consecutive tokens to be similar: | What is &quot;weight tying&quot; in a language model? Where weights of hidden-to-output layer is the same as input-to-hidden. | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Further Research . In LMModel2, why can forward start with h=0? Why don&#39;t we need to say h=torch.zeros(...)? | Write the code for an LSTM from scratch (you may refer to &lt;&gt;).&lt;/li&gt; Search the internet for the GRU architecture and implement it from scratch, and try training a model. See if you can get results similar to those we saw in this chapter. Compare you results to the results of PyTorch&#39;s built in GRU module. | Take a look at the source code for AWD-LSTM in fastai, and try to map each of the lines of code to the concepts shown in this chapter. | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; | |",
            "url": "https://usama280.github.io/PasteBlogs/2021/08/22/Lesson12-nlp_dive.html",
            "relUrl": "/2021/08/22/Lesson12-nlp_dive.html",
            "date": " • Aug 22, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Lesson 10 - FastAI",
            "content": "NLP Deep Dive: RNNs . We are now going to take a look into natural language processing. Were going to build two models: One that can predict the next word (generate text), and another that can classify if a text is positive or negative. Note: We will be using a movie review dataset for this model. . Grab path . from fastai.text.all import * path = untar_data(URLs.IMDB) #our data path . Path.BASE_PATH = path path.ls() . (#7) [Path(&#39;train&#39;),Path(&#39;imdb.vocab&#39;),Path(&#39;tmp_lm&#39;),Path(&#39;unsup&#39;),Path(&#39;tmp_clas&#39;),Path(&#39;README&#39;),Path(&#39;test&#39;)] . (path/&#39;train/pos&#39;).ls() #the path consists of text files . (#12500) [Path(&#39;train/pos/5840_7.txt&#39;),Path(&#39;train/pos/7429_9.txt&#39;),Path(&#39;train/pos/8401_10.txt&#39;),Path(&#39;train/pos/4606_7.txt&#39;),Path(&#39;train/pos/11152_10.txt&#39;),Path(&#39;train/pos/11180_7.txt&#39;),Path(&#39;train/pos/11887_8.txt&#39;),Path(&#39;train/pos/8072_10.txt&#39;),Path(&#39;train/pos/5256_10.txt&#39;),Path(&#39;train/pos/6267_10.txt&#39;)...] . files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) #lets grab the following folders . txt = files[0].open().read() txt[:75] . &#39;While the premise of the film is pretty lame (Ollie is diagnosed with &#34;horn&#39; . Word Tokenization with FastAI . To store the words, will be using a tokenizer. There are many benefits to using tokenizers as you will see below. . spacy = WordTokenizer() #our tokenizer toks = first(spacy([txt])) #tokenize our scentence and grab first print(coll_repr(toks, 30)) . (#365) [&#39;While&#39;,&#39;the&#39;,&#39;premise&#39;,&#39;of&#39;,&#39;the&#39;,&#39;film&#39;,&#39;is&#39;,&#39;pretty&#39;,&#39;lame&#39;,&#39;(&#39;,&#39;Ollie&#39;,&#39;is&#39;,&#39;diagnosed&#39;,&#39;with&#39;,&#39;&#34;&#39;,&#39;hornophobia&#39;,&#39;&#34;&#39;,&#39;)&#39;,&#39;,&#39;,&#39;the&#39;,&#39;film&#39;,&#39;is&#39;,&#39;an&#39;,&#39;amiable&#39;,&#39;and&#39;,&#39;enjoyable&#39;,&#39;little&#39;,&#39;flick&#39;,&#39;.&#39;,&#39;It&#39;...] . first(spacy([&#39;The U.S. dollar $1 is $1.00.&#39;])) . (#9) [&#39;The&#39;,&#39;U.S.&#39;,&#39;dollar&#39;,&#39;$&#39;,&#39;1&#39;,&#39;is&#39;,&#39;$&#39;,&#39;1.00&#39;,&#39;.&#39;] . Notice U.S. and 1.00 is not seperated:This is one reason why tokenizers are useful. . tkn = Tokenizer(spacy) #Wrapper that designates special tokens print(coll_repr(tkn(txt), 31)) . (#403) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;while&#39;,&#39;the&#39;,&#39;premise&#39;,&#39;of&#39;,&#39;the&#39;,&#39;film&#39;,&#39;is&#39;,&#39;pretty&#39;,&#39;lame&#39;,&#39;(&#39;,&#39;ollie&#39;,&#39;is&#39;,&#39;diagnosed&#39;,&#39;with&#39;,&#39;&#34;&#39;,&#39;hornophobia&#39;,&#39;&#34;&#39;,&#39;)&#39;,&#39;,&#39;,&#39;the&#39;,&#39;film&#39;,&#39;is&#39;,&#39;an&#39;,&#39;amiable&#39;,&#39;and&#39;,&#39;enjoyable&#39;,&#39;little&#39;,&#39;flick&#39;,&#39;.&#39;...] . xxbos:Beggining of text &gt; xxmaj:Next word was capital &gt; xxnuk:Next word is unknown &gt; xxrep:Repeated words Just know that anything that is xx___ is a special token . defaults.text_proc_rules #some more rules . [&lt;function fastai.text.core.fix_html(x)&gt;, &lt;function fastai.text.core.replace_rep(t)&gt;, &lt;function fastai.text.core.replace_wrep(t)&gt;, &lt;function fastai.text.core.spec_add_spaces(t)&gt;, &lt;function fastai.text.core.rm_useless_spaces(t)&gt;, &lt;function fastai.text.core.replace_all_caps(t)&gt;, &lt;function fastai.text.core.replace_maj(t)&gt;, &lt;function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)&gt;] . coll_repr(tkn(&#39;&amp;copy; Fast.ai www.fast.ai/INDEX&#39;), 31) . &#34;(#11) [&#39;xxbos&#39;,&#39;©&#39;,&#39;xxmaj&#39;,&#39;fast.ai&#39;,&#39;xxrep&#39;,&#39;3&#39;,&#39;w&#39;,&#39;.fast.ai&#39;,&#39;/&#39;,&#39;xxup&#39;,&#39;index&#39;]&#34; . Sidebar: Subword Tokenization . Subword tokenization is a new tokenizer that determines words not based on spaces but frequency. This is actually better than the WordTokenizer as it can determine words from character based languages that lack spaces (Ex: Chinese). . txts = L(o.open().read() for o in files[:2000]) . def subword(sz): sp = SubwordTokenizer(vocab_sz=sz) sp.setup(txts) #trains subword token for the most commonly occuring words return &#39; &#39;.join(first(sp([txt]))[:40]) . subword(1000) . &#39;▁Whil e ▁the ▁pre m ise ▁of ▁the ▁film ▁is ▁pretty ▁la me ▁( O ll ie ▁is ▁di ag no s ed ▁with ▁&#34; h or n op ho b ia &#34; ), ▁the ▁film ▁is ▁an ▁a mi&#39; . Here words that are togather (no spaces between letters) are very common:Example, &#39;the&#39; and &#39;pretty&#39; . subword(200) #lets make the word vocab smaller . &#39;▁ W h i le ▁the ▁p re m is e ▁of ▁the ▁film ▁is ▁p re t t y ▁ la m e ▁ ( O ll i e ▁is ▁d i a g n o s ed ▁with&#39; . Notice that many words have yet to be identified. (Unlike film and with) . subword(10000) #lets train a much larger word vocab . &#39;▁Whil e ▁the ▁premise ▁of ▁the ▁film ▁is ▁pretty ▁lame ▁( O ll ie ▁is ▁diagnos ed ▁with ▁&#34; h or no pho b ia &#34;) , ▁the ▁film ▁is ▁an ▁a mi able ▁and ▁enjoyable ▁little ▁flick . ▁It&#39; . End Sidebar . Numericalization with fastai . Numericalization alters tokens so that only numerical values are within the lists: These values refer to the vocab. . Example below . toks200 = txts[:200].map(tkn) toks200[0] . (#403) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;while&#39;,&#39;the&#39;,&#39;premise&#39;,&#39;of&#39;,&#39;the&#39;,&#39;film&#39;,&#39;is&#39;,&#39;pretty&#39;...] . num = Numericalize() #returns tokens in order of freq num.setup(toks200) coll_repr(num.vocab,20) . &#34;(#2200) [&#39;xxunk&#39;,&#39;xxpad&#39;,&#39;xxbos&#39;,&#39;xxeos&#39;,&#39;xxfld&#39;,&#39;xxrep&#39;,&#39;xxwrep&#39;,&#39;xxup&#39;,&#39;xxmaj&#39;,&#39;the&#39;,&#39;.&#39;,&#39;,&#39;,&#39;and&#39;,&#39;a&#39;,&#39;of&#39;,&#39;to&#39;,&#39;is&#39;,&#39;in&#39;,&#39;it&#39;,&#39;i&#39;...]&#34; . toks200.map(num)[0][:10] . TensorText([ 2, 8, 171, 9, 0, 14, 9, 29, 16, 188]) . Now lets do it on our text . tkn = Tokenizer(WordTokenizer()) toks = tkn(txt) print(coll_repr(tkn(txt), 31)) . (#403) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;while&#39;,&#39;the&#39;,&#39;premise&#39;,&#39;of&#39;,&#39;the&#39;,&#39;film&#39;,&#39;is&#39;,&#39;pretty&#39;,&#39;lame&#39;,&#39;(&#39;,&#39;ollie&#39;,&#39;is&#39;,&#39;diagnosed&#39;,&#39;with&#39;,&#39;&#34;&#39;,&#39;hornophobia&#39;,&#39;&#34;&#39;,&#39;)&#39;,&#39;,&#39;,&#39;the&#39;,&#39;film&#39;,&#39;is&#39;,&#39;an&#39;,&#39;amiable&#39;,&#39;and&#39;,&#39;enjoyable&#39;,&#39;little&#39;,&#39;flick&#39;,&#39;.&#39;...] . nums = num(toks)[:20] #numericalization nums . TensorText([ 2, 8, 171, 9, 0, 14, 9, 29, 16, 188, 1243, 33, 1244, 16, 0, 27, 24, 0, 24, 32]) . Notice that these number values refer to the vocab. Lets decode below . &#39; &#39;.join(num.vocab[o] for o in nums) #we can decode doing the following . &#39;xxbos xxmaj while the xxunk of the film is pretty lame ( ollie is xxunk with &#34; xxunk &#34; )&#39; . Creating Batches for Language Model . stream = &quot;In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we&#39;ll have another example of the PreProcessor used in the data block API. nThen we will study how we build a language model and train it for a while.&quot; tokens = tkn(stream) bs,seq_len = 6,15 d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)]) df = pd.DataFrame(d_tokens) display(HTML(df.to_html(index=False,header=None))) . xxbos | xxmaj | in | this | chapter | , | we | will | go | back | over | the | example | of | classifying | . movie | reviews | we | studied | in | chapter | 1 | and | dig | deeper | under | the | surface | . | xxmaj | . first | we | will | look | at | the | processing | steps | necessary | to | convert | text | into | numbers | and | . how | to | customize | it | . | xxmaj | by | doing | this | , | we | &#39;ll | have | another | example | . of | the | preprocessor | used | in | the | data | block | xxup | api | . | n | xxmaj | then | we | . will | study | how | we | build | a | language | model | and | train | it | for | a | while | . | . This batch is too big for our model, lets adjust it . Minibatches . Below is an example of 3 minibatches. Notice that in each following minibatch the nth row follows the previous minibatches nth row.Example, . M1: xxbos xxmaj in this chapter M2: , we will go back M3: over the example of classifying . bs,seq_len = 6,5 d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)]) df = pd.DataFrame(d_tokens) display(HTML(df.to_html(index=False,header=None))) . xxbos | xxmaj | in | this | chapter | . movie | reviews | we | studied | in | . first | we | will | look | at | . how | to | customize | it | . | . of | the | preprocessor | used | in | . will | study | how | we | build | . bs,seq_len = 6,5 d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)]) df = pd.DataFrame(d_tokens) display(HTML(df.to_html(index=False,header=None))) . , | we | will | go | back | . chapter | 1 | and | dig | deeper | . the | processing | steps | necessary | to | . xxmaj | by | doing | this | , | . the | data | block | xxup | api | . a | language | model | and | train | . bs,seq_len = 6,5 d_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)]) df = pd.DataFrame(d_tokens) display(HTML(df.to_html(index=False,header=None))) . over | the | example | of | classifying | . under | the | surface | . | xxmaj | . convert | text | into | numbers | and | . we | &#39;ll | have | another | example | . . | n | xxmaj | then | we | . it | for | a | while | . | . nums200 = toks200.map(num) . dl = LMDataLoader(nums200) . This dataloader takes care of creating the appropraite minibatches for us . x,y = first(dl) x.shape,y.shape . (torch.Size([64, 72]), torch.Size([64, 72])) . 64 is batchsize, 72 is the seq length . &#39; &#39;.join(num.vocab[o] for o in x[0][:20]) . &#39;xxbos xxmaj while the xxunk of the film is pretty lame ( ollie is xxunk with &#34; xxunk &#34; )&#39; . &#39; &#39;.join(num.vocab[o] for o in y[0][:20]) . &#39;xxmaj while the xxunk of the film is pretty lame ( ollie is xxunk with &#34; xxunk &#34; ) ,&#39; . Notice that the label is offset by 1 word:This is what we want . Language Model Using DataBlock . get_imdb = partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) dblock = DataBlock( blocks=TextBlock.from_folder(path, is_lm=True), get_items=get_imdb, splitter=RandomSplitter(0.1) ) . dls_lm = dblock.dataloaders(path, path=path, bs=128, seq_len=80) . dls_lm.show_batch(max_n=2) . text text_ . 0 xxbos i strongly disagree with &quot; xxunk &quot; regarding xxmaj jim xxmaj belushi &#39;s talent . i happen to like xxmaj belushi very much . xxmaj admittedly , i was skeptical when he first appeared on the scene , because i was such a xxup huge fan of his late brother xxmaj john . xxmaj but xxmaj jim has an on - screen charm that has gotten him very far -- and he has developed it well over the years | i strongly disagree with &quot; xxunk &quot; regarding xxmaj jim xxmaj belushi &#39;s talent . i happen to like xxmaj belushi very much . xxmaj admittedly , i was skeptical when he first appeared on the scene , because i was such a xxup huge fan of his late brother xxmaj john . xxmaj but xxmaj jim has an on - screen charm that has gotten him very far -- and he has developed it well over the years . | . 1 is awesome . xxmaj there are some parts where you start to doubt whether the director intended to convey the message that showmanship is highly important thing in the future ( we will do such kind on corny sf things because we xxup can ) or is it simply over combining . xxmaj but the paranoia is there and feeling &quot; out of joint &quot; also . xxmaj good one . xxbos xxmaj first of all , the film is | awesome . xxmaj there are some parts where you start to doubt whether the director intended to convey the message that showmanship is highly important thing in the future ( we will do such kind on corny sf things because we xxup can ) or is it simply over combining . xxmaj but the paranoia is there and feeling &quot; out of joint &quot; also . xxmaj good one . xxbos xxmaj first of all , the film is very | . show_batch denumericalize for us, but in reality its numericalized. See below . dls_lm.one_batch()[0] . LMTensorText([[ 2, 8, 121, ..., 42, 13, 190], [ 23, 9, 522, ..., 13, 9706, 359], [35022, 48, 121, ..., 15, 159, 10], ..., [ 2202, 8, 22400, ..., 6995, 13, 650], [33649, 8, 2712, ..., 14, 21, 898], [ 16, 36, 10, ..., 28, 45, 734]], device=&#39;cuda:0&#39;) . Training . learn = language_model_learner( dls_lm, AWD_LSTM, #AWD_LSTM is a precreated architecture drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16() . learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.120048 | 3.912788 | 0.299565 | 50.038246 | 11:39 | . 30% accuracy is actually not bad . Saving and Loading Models . learn.save(&#39;1epoch&#39;) . learn = learn.load(&#39;1epoch&#39;) . Further training . learn.unfreeze() learn.fit_one_cycle(10, 2e-3) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.893486 | 3.772820 | 0.317104 | 43.502548 | 12:37 | . 1 | 3.820479 | 3.717197 | 0.323790 | 41.148880 | 12:30 | . 2 | 3.735622 | 3.659760 | 0.330321 | 38.851997 | 12:09 | . 3 | 3.677086 | 3.624794 | 0.333960 | 37.516987 | 12:12 | . 4 | 3.636646 | 3.601300 | 0.337017 | 36.645859 | 12:05 | . 5 | 3.553636 | 3.584241 | 0.339355 | 36.026001 | 12:04 | . 6 | 3.507634 | 3.571892 | 0.341353 | 35.583862 | 12:08 | . 7 | 3.444101 | 3.565988 | 0.342194 | 35.374371 | 12:08 | . 8 | 3.398597 | 3.566283 | 0.342647 | 35.384815 | 12:11 | . 9 | 3.375563 | 3.568166 | 0.342528 | 35.451500 | 12:05 | . learn.save_encoder(&#39;finetuned&#39;) #This saves the model without the final layer . Testing model . We can now test our model. Because our model is made to predict the next word, it can generate text given any input. This generated text is the prediction of the model. . TEXT = &quot;I liked this movie because&quot; N_WORDS = 40 N_SENTENCES = 2 #lets generate 2 sentences #randomness preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)] . print(&quot; n&quot;.join(preds)) . i liked this movie because of its story and characters . The story line was very strong , very good for a sci - fi film . The main character , Alucard , was very well developed and brought the whole story i liked this movie because i like the idea of the premise of the movie , the ( very ) convenient virus ( which , when you have to kill a few people , the &#34; evil &#34; machine has to be used to protect . Text Classifier . Now lets create a text classifier, which can classify if the text is positive or negative. For this we will be using our pretrained model that we saved above. . Classifier DataBlock . dblock = DataBlock( blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) #splits via folder name ) . dls_clas = dblock.dataloaders(path, path=path, bs=128, seq_len=72) . dls_clas.show_batch(max_n=3) . text category . 0 xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero | pos | . 1 xxbos * ! ! - xxup spoilers - ! ! * n n xxmaj before i begin this , let me say that i have had both the advantages of seeing this movie on the big screen and of having seen the &quot; authorized xxmaj version &quot; of this movie , remade by xxmaj stephen xxmaj king , himself , in 1997 . n n xxmaj both advantages made me appreciate this version of &quot; the xxmaj shining , &quot; all the more . n n xxmaj also , let me say that xxmaj i &#39;ve read xxmaj mr . xxmaj king &#39;s book , &quot; the xxmaj shining &quot; on many occasions over the years , and while i love the book and am a huge fan of his work , xxmaj stanley xxmaj kubrick &#39;s retelling of this story is far more compelling … and xxup scary . n n xxmaj kubrick | pos | . 2 xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review n n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , steaming bowl of oatmeal . xxmaj it &#39;s warm and gooey , but you &#39;re not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj victor xxmaj vargas became i was always aware that something did n&#39;t quite feel right . xxmaj victor xxmaj vargas suffers from a certain overconfidence on the director &#39;s part . xxmaj apparently , the director thought that the ethnic backdrop of a xxmaj latino family on the lower east side , and an idyllic storyline would make the film critic proof . xxmaj he was right , but it did n&#39;t fool me . xxmaj raising xxmaj victor xxmaj vargas is | neg | . nums_samp = toks200[:10].map(num) #lets grab some reviews . nums_samp.map(len) . (#10) [403,176,151,63,185,905,417,97,183,397] . Notice they vary in lengths. This can be a problem, however, FastAI DataBlock takes care of it for us by using padding . learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16() . learn = learn.load_encoder(&#39;finetuned&#39;) #lets load our model from before . Fine-Tuning the Classifier . learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.347427 | 0.184480 | 0.929320 | 00:33 | . Notice how quickly it trained:This is the benefit of using pretrained models and only fitting on the final layer. . Refining . Lets refine the model by training some more. For NLP it&#39;s better to only freeze a couple of layers at a time, rather than the entire thing. So, below we can do this by calling .freeze_to(-2), which freeze all except the last two parameter groups: . learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) . epoch train_loss valid_loss accuracy time . 0 | 0.247763 | 0.171683 | 0.934640 | 00:37 | . Then we can unfreeze a bit more, and continue training: . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.193377 | 0.156696 | 0.941200 | 00:45 | . And finally, the whole model! . learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.172888 | 0.153770 | 0.943120 | 01:01 | . 1 | 0.161492 | 0.155567 | 0.942640 | 00:57 | . This accuracy is very good! . Conclusion . Overall, natural language models are very powerful and benefitial. Hopefully, you learned how to create a model that can generate text, and more importantly, can be used to classify text as well (Transfer learning). . Questionnaire . What is &quot;self-supervised learning&quot;? Learning where model has no labels. | What is a &quot;language model&quot;? A language model is a model that tries to predict the next word in a text. | Why is a language model considered self-supervised? Because it does not require any labels needed to learn. | What are self-supervised models usually used for? Often they are used as pre-trained model for transfer learning. | Why do we fine-tune language models? By finetuning (final layers) we can fit a model to our data. Note, this assumes the data being fit on is similer. | What are the three steps to create a state-of-the-art text classifier? Train a language model Finetune language model on classification dataset Finetune further as classifier | How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset? It has been trained to predict the next word: To do this, the model understands the language (Ex: sentiment). | What are the three steps to prepare your data for a language model? Tokenization Numericalization DataLoader | What is &quot;tokenization&quot;? Why do we need it? Tokenization splits words into a list: However, it&#39;s not that simple as it is vary of punctuations, syntax, etc. | Name three different approaches to tokenization. Word-based tokenization Subword-based tokenization Character-based tokenization | What is xxbos? Beginning of text | List four rules that fastai applies to text during tokenization. xxrep, xxbox, xxcap, xxeos | Why are repeated characters replaced with a token showing the number of repetitions and the character that&#39;s repeated? We can expect that repeated characters have special or different meaning than just a single character: Hence, why it is better to use a token to repersent this distinction. | What is &quot;numericalization&quot;? The mapping of values to vocab | Why might there be words that are replaced with the &quot;unknown word&quot; token? Such words make the embedding matrix far too large and increase memory usage. | With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book&#39;s website.) Minibatch of the nth row follows the previous minibatches nth row. | Why do we need padding for text classification? Why don&#39;t we need it for language modeling? Padding is needed because each text is of different sizes. It is not required for language modeling as the documents are all concatenated. | What does an embedding matrix for NLP contain? What is its shape? It contains vector representations of all tokens in the vocabulary. The embedding matrix has the size vocab_size x embedding_size. | What is &quot;perplexity&quot;? Exponential of the loss. | Why do we have to pass the vocabulary of the language model to the classifier data block? We need the vocab correspondence of tokens to index to remain the same because we used the pretrained language model. | What is &quot;gradual unfreezing&quot;? The unfreezing of one layer at a time and fine-tuning. | Why is text generation always likely to be ahead of automatic identification of machine-generated texts? The text generation model could be made so that it competes with the identification model. Eventually, the text generation will produce text that the identification model cannot identify as being machine-generated. | Further Research . See what you can learn about language models and disinformation. What are the best language models today? Take a look at some of their outputs. Do you find them convincing? How could a bad actor best use such a model to create conflict and uncertainty? | Given the limitation that models are unlikely to be able to consistently recognize machine-generated texts, what other approaches may be needed to handle large-scale disinformation campaigns that leverage deep learning? |",
            "url": "https://usama280.github.io/PasteBlogs/2021/08/22/Lesson10-nlp.html",
            "relUrl": "/2021/08/22/Lesson10-nlp.html",
            "date": " • Aug 22, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "EmbeddingRandomForest",
            "content": "Random Forest with Embeddings . So far we have created both a random forest and a NN to do tabular modeling. One thing intresting about a NN is that it contains embeddings. Why don&#39;t we try to use these embeddings from the Neural Network in Random Forests? Will it improve the random forest? Lets find out! . Unzipping data . import zipfile z= zipfile.ZipFile(&#39;bluebook-for-bulldozers.zip&#39;) #unzip first z.extractall() #extract . Grabbing the Data . Similer to what we did in lesson 9, we will grab our data, set the ordinal var, and feature engineer the date. . df_nn = pd.read_csv(Path()/&#39;TrainAndValid.csv&#39;, low_memory=False) #Data #Set ordinal variables using our order sizes = &#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39; df_nn[&#39;ProductSize&#39;] = df_nn[&#39;ProductSize&#39;].astype(&#39;category&#39;) df_nn[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) dep_var = &#39;SalePrice&#39; df_nn[dep_var] = np.log(df_nn[dep_var]) #remember we need to take log of the label (Kaggle requires) df_nn = add_datepart(df_nn, &#39;saledate&#39;) #Also remember that we used feature engineering on date . Continous and Categorical columns . cont_nn,cat_nn = cont_cat_split(df_nn, max_card=9000, dep_var=dep_var) #Max_card makes it so that any col with more than # 9000 lvls, it will be treated as cont . cont_nn . [&#39;SalesID&#39;, &#39;MachineID&#39;, &#39;auctioneerID&#39;, &#39;MachineHoursCurrentMeter&#39;] . Notice that it&#39;s missing saleElpased from the cont_nn. We need to add this as we want this col to be treated as cont. . cont_nn.append(&#39;saleElapsed&#39;) cont_nn . [&#39;SalesID&#39;, &#39;MachineID&#39;, &#39;auctioneerID&#39;, &#39;MachineHoursCurrentMeter&#39;, &#39;saleElapsed&#39;] . cat_nn.remove(&#39;saleElapsed&#39;) . df_nn.dtypes[&#39;saleElapsed&#39;] #must change to int as an object type will cause error . dtype(&#39;O&#39;) . df_nn[&#39;saleElapsed&#39;] = df_nn[&#39;saleElapsed&#39;].astype(&#39;int&#39;) . Split . We want to split our data by date, not randomly. . cond = (df_nn.saleYear&lt;2011) | (df_nn.saleMonth&lt;10) train_idx = np.where( cond)[0] valid_idx = np.where(~cond)[0] splits = (list(train_idx),list(valid_idx)) . Tabular object . Now that we have everything we need, lets create our tabular object. . procs_nn = [Categorify, FillMissing, Normalize] to_nn = TabularPandas(df_nn, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var) . dls = to_nn.dataloaders(1024) #minibatches . y = to_nn.train.y y.min(),y.max() . (8.465899, 11.863583) . Training . learn = tabular_learner(dls, y_range=(8,12), layers=[500,250], n_out=1, loss_func=F.mse_loss) learn.lr_find() #find best lr . SuggestedLRs(lr_min=0.0033113110810518267, lr_steep=0.00019054606673307717) . learn.fit_one_cycle(5, 1e-2) #train . epoch train_loss valid_loss time . 0 | 0.058050 | 0.054965 | 00:14 | . 1 | 0.047368 | 0.052232 | 00:14 | . 2 | 0.041544 | 0.050312 | 00:14 | . 3 | 0.035930 | 0.049067 | 00:14 | . 4 | 0.031330 | 0.049272 | 00:14 | . def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6) def m_rmse(m, xs, y): return r_mse(m.predict(xs), y) . preds,targs = learn.get_preds() r_mse(preds,targs) . 0.221972 . This is actually very good . Random Forest . Lets now create our random forest and compare it to our NN . Tabular object . procs = [Categorify, FillMissing] rf_to = TabularPandas(df_nn, procs, cat_nn, cont_nn, y_names=dep_var, splits=splits) . xs,y = rf_to.train.xs,rf_to.train.y valid_xs,valid_y = rf_to.valid.xs,rf_to.valid.y . Random Forest . def rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) . m = rf(xs, y) #Fitting . m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) . (0.171432, 0.233555) . So it seems our random forest preformed worse in comparison to the NN. Let&#39;s improve this by adding the NN embeddings! . Adding embeddings . learn.model.embeds[:5] #These are just some of the embedding within the NN . ModuleList( (0): Embedding(54, 15) (1): Embedding(5242, 194) (2): Embedding(7, 5) (3): Embedding(73, 18) (4): Embedding(4, 3) ) . The function below extracts the embeddings from the model . def embed_features(learner, xs): xs = xs.copy() for i, feature in enumerate(learn.dls.cat_names): emb = learner.model.embeds[i].cpu() new_feat = pd.DataFrame(emb(tensor(xs[feature], dtype=torch.int64)), index=xs.index, columns=[f&#39;{feature}_{j}&#39; for j in range(emb.embedding_dim)]) xs.drop(columns=feature, inplace=True) xs = xs.join(new_feat) return xs . embeded_xs = embed_features(learn, learn.dls.train.xs) xs_valid = embed_features(learn, learn.dls.valid.xs) . embeded_xs.shape, xs_valid.shape . ((404710, 907), (7988, 907)) . Fitting embeddings . Now that we have our embeddings, lets fit it into the random forest. . m = rf(embeded_xs, y) #Fitting . m_rmse(m, embeded_xs, y), m_rmse(m, xs_valid, valid_y) . (0.14817, 0.228745) . It seems that adding the NN embeddings improves the random forest! .",
            "url": "https://usama280.github.io/PasteBlogs/2021/08/22/EmbeddingRandomForest.html",
            "relUrl": "/2021/08/22/EmbeddingRandomForest.html",
            "date": " • Aug 22, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Cross Entropy Collab",
            "content": "Collaborative Filtering . The collaborative filtering problem we will be doing is one we have done before using the MSE Loss. But, what if try doing it using cross entropy loss, how will that affect the model? Lets find out! . from fastai.collab import * from fastai.tabular.all import * . Data . Lets grab the data and view it . path = untar_data(URLs.ML_100k) ratings = pd.read_csv(path/&#39;u.data&#39;, delimiter=&#39; t&#39;, header=None, names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) ratings.head() . user movie rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . DataSets . We need to convert our data into a DataBlock. But before that, lets try making a DataSets and forming our x,y. . dblock = DataBlock() dsets = dblock.datasets(ratings) x,y = dsets.train[0] x,y . (user 272 movie 1101 rating 5 timestamp 879454977 Name: 1509, dtype: int64, user 272 movie 1101 rating 5 timestamp 879454977 Name: 1509, dtype: int64) . x[&#39;user&#39;], x[&#39;movie&#39;] . (272, 1101) . This the data we want to feed . y[&#39;rating&#39;] . 5 . This is our label, the rating . DataBlock . Lets put it all togather and create our datablock . def get_x(rating): return tensor([rating[&#39;user&#39;]-1, rating[&#39;movie&#39;]-1]) #Must sub 1 to avoid CUDA error: device-side assert triggered def get_y(rating): return rating[&#39;rating&#39;]-1 #Must sub 1 to avoid CUDA error: device-side assert triggered dblock = DataBlock(get_x=get_x, get_y=get_y, splitter=RandomSplitter()) dls = dblock.dataloaders(ratings) #path . Class and methods . Lets put all the functions under a class and have it extend the Module class. . class CollabClassification(Module): def __init__(self, users_sz, movies_sz, n_factors = 100): self.user_factors = Embedding(*users_sz) self.movie_factors = Embedding(*movies_sz) self.layers = nn.Sequential( nn.Linear(users_sz[1] + movies_sz[1], n_factors), nn.ReLU(), nn.Linear(n_factors, 5) #5 output neurons ) def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return self.layers(torch.cat((users, movies), dim=1)) . n_users = len(ratings.user.unique()) n_movies = len(ratings.movie.unique()) users_factors = 74 #random movies_factors = 102 #random . model = CollabClassification((n_users, users_factors), (n_movies, movies_factors)) #our model . Training . learn = Learner(dls, model, loss_func=CrossEntropyLossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) . epoch train_loss valid_loss time . 0 | 1.292975 | 1.282268 | 00:15 | . 1 | 1.234408 | 1.255242 | 00:14 | . 2 | 1.189355 | 1.235191 | 00:14 | . 3 | 1.158378 | 1.231530 | 00:14 | . 4 | 1.095451 | 1.245237 | 00:14 | . It didn&#39;t perform that well in comparison to the MSE Loss model, but that is to be expected as this problem is not advised to be handled using cross entropy loss. .",
            "url": "https://usama280.github.io/PasteBlogs/2021/08/22/CrossEntropyCollab.html",
            "relUrl": "/2021/08/22/CrossEntropyCollab.html",
            "date": " • Aug 22, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Bear Classifier",
            "content": "Goal . Lets create a bear classifier that can classify Teddy, Black, and Grizzly bears. . First lets get the training data . from pathlib import Path root = Path().cwd()/&quot;images&quot; #rmtree(root) #Deletes all previous images from jmd_imagescraper.core import * duckduckgo_search(root, &quot;Grizzly&quot;, &quot;Grizzly bears&quot;, max_results=300) duckduckgo_search(root, &quot;Black&quot;, &quot;Black bears&quot;, max_results=300) duckduckgo_search(root, &quot;Teddy&quot;, &quot;Teddy bears&quot;, max_results=300) #duckduckgo_search(root, &quot;Random&quot;, &quot;Random images&quot;, max_results=300) . Lets view the data . from jmd_imagescraper.imagecleaner import * display_image_cleaner(root) . Creating datablock . bears = DataBlock( blocks=(ImageBlock, CategoryBlock), #Independent are images, dependent is the categories get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = bears.dataloaders(root) . dls.valid.show_batch(max_n=4, nrows=1) #Viewing data . Creating model and training . learner = cnn_learner(dls, resnet18, metrics = accuracy) learner.fine_tune(2) . epoch train_loss valid_loss accuracy time . 0 | 0.955830 | 0.151302 | 0.961111 | 00:09 | . epoch train_loss valid_loss accuracy time . 0 | 0.198822 | 0.065566 | 0.988889 | 00:11 | . 1 | 0.122963 | 0.038051 | 0.994444 | 00:11 | . Now lets export this model . We should export this model, in case we want to use it again. . learner.export(fname=&#39;bear.pkl&#39;) . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#2) [Path(&#39;bear.pkl&#39;),Path(&#39;export.pkl&#39;)] . Lets test it with our images . learn_inf = load_learner(path/&#39;bear.pkl&#39;) . uploader = widgets.FileUpload() uploader . img = PILImage.create(uploader.data[0]) img.to_thumb(192) . learn_inf.predict(img) . (&#39;Teddy&#39;, TensorImage(2), TensorImage([7.3293e-08, 9.1892e-06, 9.9999e-01])) . It got it correct! . Now lets try an image with more than 1 category . uploader = widgets.FileUpload() uploader . img = PILImage.create(uploader.data[0]) img.to_thumb(192) . learn_inf.predict(img) . (&#39;Grizzly&#39;, TensorImage(1), TensorImage([2.9330e-01, 7.0659e-01, 1.1200e-04])) . It only got 1 correct - This is because our model isn&#39;t made to do multi-labels . Lets make it so our model can do multi-label . Remember to make a multi-label classifier out DataBlock needs to be adjusted: MultiCategoryBlock . def parent_label_multi(o): return [Path(o).parent.name] bears2 = DataBlock( blocks=(ImageBlock, MultiCategoryBlock), #Independent are images, dependent is the multiple labels get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y= parent_label_multi, item_tfms=Resize(128)) dls2 = bears2.dataloaders(root) . dls2.show_batch() . dls2.vocab . [&#39;Black&#39;, &#39;Grizzly&#39;, &#39;Teddy&#39;] . learn2 = cnn_learner(dls2, resnet18, metrics=partial(accuracy_multi, thresh=0.2)) #lets just pick a tresh of .2 learn2.fine_tune(2) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.760363 | 0.213807 | 0.840741 | 00:04 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.299894 | 0.143817 | 0.892593 | 00:04 | . 1 | 0.211261 | 0.125826 | 0.911111 | 00:04 | . Lets test it with the same image again . uploader = widgets.FileUpload() uploader . img = PILImage.create(uploader.data[0]) img.to_thumb(200) . learn2.predict(img) . ((#2) [&#39;Black&#39;,&#39;Grizzly&#39;], TensorImage([ True, True, False]), TensorImage([6.0354e-01, 9.9354e-01, 1.6160e-04])) . It got both of them right! . Lets test it with a random image . uploader = widgets.FileUpload() uploader . img = PILImage.create(uploader.data[0]) img.to_thumb(200) . learn2.predict(img) #It works . ((#0) [], TensorImage([False, False, False]), TensorImage([0.0265, 0.4757, 0.3557])) . Notice that there is no prediction! This is the other benifit of a multi-label classifier! . Now lets make this into an application/website . btn_upload = widgets.FileUpload() out_pl = widgets.Output() lbl_pred = widgets.Label() btn_run = widgets.Button(description=&#39;Classify&#39;) . Method for what to do on click . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . btn_run.on_click(on_click_classify) . VBox used to put all widgets togather . VBox([widgets.Label(&#39;Select your bear!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) .",
            "url": "https://usama280.github.io/PasteBlogs/2021/08/22/Bear-classifier.html",
            "relUrl": "/2021/08/22/Bear-classifier.html",
            "date": " • Aug 22, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Lesson 9 - FastAI",
            "content": "from fastbook import * from kaggle import api from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype from fastai.tabular.all import * from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor from dtreeviz.trees import * from IPython.display import Image, display_svg, SVG pd.options.display.max_rows = 20 pd.options.display.max_columns = 8 . !echo &#39;{&quot;username&quot;:&quot;unadeem&quot;,&quot;key&quot;:&quot;ed1df5a2cd97f9d82e42c37511c02095&quot;}&#39; &gt; /root/.kaggle/kaggle.json . !kaggle competitions download -c bluebook-for-bulldozers #Downloading data from Kaggle . bluebook-for-bulldozers.zip: Skipping, found more recently modified local copy (use --force to force download) . Tabular Modeling Deep Dive . We will be creating a tabular model that predicts salesprice. . Unzipping data . import zipfile z= zipfile.ZipFile(&#39;bluebook-for-bulldozers.zip&#39;) #unzip first z.extractall() #extract . Look at the Data . df = pd.read_csv(&#39;TrainAndValid.csv&#39;, low_memory=False) #get data from csv file . df.columns . Index([&#39;SalesID&#39;, &#39;SalePrice&#39;, &#39;MachineID&#39;, &#39;ModelID&#39;, &#39;datasource&#39;, &#39;auctioneerID&#39;, &#39;YearMade&#39;, &#39;MachineHoursCurrentMeter&#39;, &#39;UsageBand&#39;, &#39;saledate&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;fiSecondaryDesc&#39;, &#39;fiModelSeries&#39;, &#39;fiModelDescriptor&#39;, &#39;ProductSize&#39;, &#39;fiProductClassDesc&#39;, &#39;state&#39;, &#39;ProductGroup&#39;, &#39;ProductGroupDesc&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;, &#39;Forks&#39;, &#39;Pad_Type&#39;, &#39;Ride_Control&#39;, &#39;Stick&#39;, &#39;Transmission&#39;, &#39;Turbocharged&#39;, &#39;Blade_Extension&#39;, &#39;Blade_Width&#39;, &#39;Enclosure_Type&#39;, &#39;Engine_Horsepower&#39;, &#39;Hydraulics&#39;, &#39;Pushblock&#39;, &#39;Ripper&#39;, &#39;Scarifier&#39;, &#39;Tip_Control&#39;, &#39;Tire_Size&#39;, &#39;Coupler&#39;, &#39;Coupler_System&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;Track_Type&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Stick_Length&#39;, &#39;Thumb&#39;, &#39;Pattern_Changer&#39;, &#39;Grouser_Type&#39;, &#39;Backhoe_Mounting&#39;, &#39;Blade_Type&#39;, &#39;Travel_Controls&#39;, &#39;Differential_Type&#39;, &#39;Steering_Controls&#39;], dtype=&#39;object&#39;) . df[&#39;ProductSize&#39;].unique() #Lets view this oridnal colomn . array([nan, &#39;Medium&#39;, &#39;Small&#39;, &#39;Large / Medium&#39;, &#39;Mini&#39;, &#39;Large&#39;, &#39;Compact&#39;], dtype=object) . Notice that these seem to be in random order, we should fix this as order has value . sizes = &#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39; #our desired order . df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].astype(&#39;category&#39;) #Turn into categorical variable df[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) #Now we can set our order . df[&#39;ProductSize&#39;].unique() . [NaN, &#39;Medium&#39;, &#39;Small&#39;, &#39;Large / Medium&#39;, &#39;Mini&#39;, &#39;Large&#39;, &#39;Compact&#39;] Categories (6, object): [&#39;Large&#39; &lt; &#39;Large / Medium&#39; &lt; &#39;Medium&#39; &lt; &#39;Small&#39; &lt; &#39;Mini&#39; &lt; &#39;Compact&#39;] . dep_var = &#39;SalePrice&#39; #our label . df[dep_var] = np.log(df[dep_var]) #kaggle wants us to do MNSLE, so we must take log . Decision Trees . A decision tree asks a series of binary question (Yes or No) about the data. Below is an image demostrating a dicision tree: . . We actually don&#39;t know how to come up with these question, so we have the model do it for us! . Handling Dates . We need to handle dates a little differently. Because they don&#39;t provide much information, we use feature engineering to create my specific categories. Feature engineering is taken care of by FastAI ToolKit add_datepart. . df = add_datepart(df, &#39;saledate&#39;) . &#39; &#39;.join(o for o in df.columns if o.startswith(&#39;sale&#39;)) . &#39;saleWeek saleYear saleMonth saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed&#39; . Notice the many distinct categories . df_test = pd.read_csv(Path()/&#39;Test.csv&#39;, low_memory=False) df_test = add_datepart(df_test, &#39;saledate&#39;) #Doing the same for test dataset . Using TabularPandas and TabularProc . We actually need to clean our data a little more. Will be using a transform, TabularProc: Specifically Categorify and FillMissing. Categorify replaces columns with numerical categorical columns, and FillMissing replaces missing values with the median of the column. . procs = [Categorify, FillMissing] . Creating our validation set . Below we want to split our data by date, where the last couple of weeks of data is used for the validation set. We want to split our data by date this time and not randomly because date is a major factor in the prediction. . cond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10) train_idx = np.where( cond)[0] valid_idx = np.where(~cond)[0] splits = (list(train_idx),list(valid_idx)) . len(cond), len(train_idx), len(valid_idx) . (412698, 404710, 7988) . Seperating data . We also need to seperate the continous data from the categorical. We can also do this using FastAI . cont,cat = cont_cat_split(df, 1, dep_var=dep_var) #Also pass our label so it isnt included . cont[:4] . [&#39;SalesID&#39;, &#39;MachineID&#39;, &#39;ModelID&#39;, &#39;datasource&#39;] . cat[:4] . [&#39;saleWeek&#39;, &#39;UsageBand&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;] . TabularPandas . Now we can take everything we did above and throw it into the Tabular Object . to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) . len(to.train),len(to.valid) . (404710, 7988) . to.show(3) #We can view our data similer to DataLoader . saleWeek UsageBand fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Hydraulics_Flow Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed auctioneerID_na MachineHoursCurrentMeter_na SalesID MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter saleYear saleMonth saleDay saleDayofweek saleDayofyear SalePrice . 0 46 | Low | 521D | 521 | D | #na# | #na# | #na# | Wheel Loader - 110.0 to 120.0 Horsepower | Alabama | WL | Wheel Loader | #na# | EROPS w AC | None or Unspecified | #na# | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | 2 Valve | #na# | #na# | #na# | #na# | None or Unspecified | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Standard | Conventional | False | False | False | False | False | False | 1163635200 | False | False | 1139246 | 999089 | 3157 | 121 | 3.0 | 2004 | 68.0 | 2006 | 11 | 16 | 3 | 320 | NaN | . 1 13 | Low | 950FII | 950 | F | II | #na# | Medium | Wheel Loader - 150.0 to 175.0 Horsepower | North Carolina | WL | Wheel Loader | #na# | EROPS w AC | None or Unspecified | #na# | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | 2 Valve | #na# | #na# | #na# | #na# | 23.5 | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Standard | Conventional | False | False | False | False | False | False | 1080259200 | False | False | 1139248 | 117657 | 77 | 121 | 3.0 | 1996 | 4640.0 | 2004 | 3 | 26 | 4 | 86 | NaN | . 2 9 | High | 226 | 226 | #na# | #na# | #na# | #na# | Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity | New York | SSL | Skid Steer Loaders | #na# | OROPS | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Auxiliary | #na# | #na# | #na# | #na# | #na# | None or Unspecified | None or Unspecified | None or Unspecified | Standard | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | False | False | False | False | False | False | 1077753600 | False | False | 1139249 | 434808 | 7009 | 121 | 3.0 | 2001 | 2838.0 | 2004 | 2 | 26 | 3 | 57 | NaN | . to.items.head(3) . SalesID SalePrice MachineID saleWeek ... saleIs_year_start saleElapsed auctioneerID_na MachineHoursCurrentMeter_na . 0 1139246 | NaN | 999089 | 46 | ... | 1 | 2647 | 1 | 1 | . 1 1139248 | NaN | 117657 | 13 | ... | 1 | 2148 | 1 | 1 | . 2 1139249 | NaN | 434808 | 9 | ... | 1 | 2131 | 1 | 1 | . 3 rows × 67 columns . to1 = TabularPandas(df, procs, [&#39;state&#39;, &#39;ProductGroup&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;], [], y_names=dep_var, splits=splits) to1.show(3) . state ProductGroup Drive_System Enclosure SalePrice . 0 Alabama | WL | #na# | EROPS w AC | NaN | . 1 North Carolina | WL | #na# | EROPS w AC | NaN | . 2 New York | SSL | #na# | OROPS | NaN | . It shows strings, but it&#39;s actually stored internally as digits. See below: . to1.items[[&#39;state&#39;, &#39;ProductGroup&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;]].head(3) . state ProductGroup Drive_System Enclosure . 0 1 | 6 | 0 | 3 | . 1 33 | 6 | 0 | 3 | . 2 32 | 3 | 0 | 6 | . These values are referring to the vocab . to.classes[&#39;ProductSize&#39;] . [&#39;#na#&#39;, &#39;Large&#39;, &#39;Large / Medium&#39;, &#39;Medium&#39;, &#39;Small&#39;, &#39;Mini&#39;, &#39;Compact&#39;] . save_pickle(Path()/&#39;to.pkl&#39;,to) . Lets also export what we currently have, so we don&#39;t have to rerun the processing . Creating the Decision Tree . xs,y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y . m = DecisionTreeRegressor(max_leaf_nodes=4) #Creating tree m.fit(xs, y) #Fitting . DecisionTreeRegressor(max_leaf_nodes=4) . draw_tree(m, xs, size=10, leaves_parallel=True, precision=2) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 Coupler_System ≤ 0.5 mse = 0.48 samples = 404710 value = 10.1 1 YearMade ≤ 1991.5 mse = 0.42 samples = 360847 value = 10.21 0&#45;&gt;1 True 2 mse = 0.12 samples = 43863 value = 9.21 0&#45;&gt;2 False 3 mse = 0.37 samples = 155724 value = 9.97 1&#45;&gt;3 4 ProductSize ≤ 4.5 mse = 0.37 samples = 205123 value = 10.4 1&#45;&gt;4 5 mse = 0.31 samples = 182403 value = 10.5 4&#45;&gt;5 6 mse = 0.17 samples = 22720 value = 9.62 4&#45;&gt;6 We can also view the above information using another library . samp_idx = np.random.permutation(len(y))[:500] dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, &#39;SalePrice&#39;, fontname=&#39;DejaVu Sans&#39;, scale=1.6, label_fontsize=10, orientation=&#39;LR&#39;) . G node4 2021-07-17T16:06:44.700447 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ leaf5 2021-07-17T16:06:45.142017 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ node4-&gt;leaf5 leaf6 2021-07-17T16:06:45.236493 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ node4-&gt;leaf6 node1 2021-07-17T16:06:44.810921 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ node1-&gt;node4 leaf3 2021-07-17T16:06:45.049515 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ node1-&gt;leaf3 leaf2 2021-07-17T16:06:45.322526 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ node0 2021-07-17T16:06:44.932606 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ node0-&gt;node1 &#8804; node0-&gt;leaf2 &gt; Notice that here we can see that their are models made in the 1000&#39;s which is obv not true:We can fix this . xs.loc[xs[&#39;YearMade&#39;]&lt;1900, &#39;YearMade&#39;] = 1950 valid_xs.loc[valid_xs[&#39;YearMade&#39;]&lt;1900, &#39;YearMade&#39;] = 1950 . m = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y) dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, &#39;SalePrice&#39;, fontname=&#39;DejaVu Sans&#39;, scale=1.6, label_fontsize=10, orientation=&#39;LR&#39;) . G node4 2021-07-17T16:17:29.120981 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ leaf5 2021-07-17T16:17:29.826782 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ node4-&gt;leaf5 leaf6 2021-07-17T16:17:29.918891 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ node4-&gt;leaf6 node1 2021-07-17T16:17:29.500113 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ node1-&gt;node4 leaf3 2021-07-17T16:17:29.738221 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ node1-&gt;leaf3 leaf2 2021-07-17T16:17:30.000175 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ node0 2021-07-17T16:17:29.621002 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ node0-&gt;node1 &#8804; node0-&gt;leaf2 &gt; Now it looks a little cleaner . m = DecisionTreeRegressor() m.fit(xs, y); . def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6) def m_rmse(m, xs, y): return r_mse(m.predict(xs), y) . Lets test our model . m_rmse(m, xs, y) . 0.0 . We got 0! Does that mean our model is perfect? No, as you will see below the validation performs worse. We are overfitting. . m_rmse(m, valid_xs, valid_y) . 0.332418 . m.get_n_leaves(), len(xs) #lets view how many leaves we have . (324542, 404710) . We seem to have just as much leave nodes as data . Lets fix our tree and retrain . Lets change the hyperparameter and see if it improves the performance. . m = DecisionTreeRegressor(min_samples_leaf=25) m.fit(to.train.xs, to.train.y) m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) . (0.248593, 0.323419) . m.get_n_leaves() . 12397 . It seems like we are just guessing, maybe there is a better way to choose (Similer to the LR finder). . Random Forests . This idea is similer to LR finder, where we take minibatches and train it. Here we subset the data items (the rows) and we subset the fields (the columns). . Creating a Random Forest . n_estimators defines the number of trees we want, max_samples defines how many rows to sample for training each tree, and max_features defines how many columns to sample at each split point (where 0.5 means &quot;take half the total number of columns&quot;). We can also specify when to stop splitting the tree nodes by including min_samples_leaf. Finally, we pass n_jobs=-1 to tell sklearn to use all our CPUs to build the trees in parallel. . def rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) . m = rf(xs, y); . m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) . (0.170752, 0.232851) . Our valid has improved . preds = np.stack([t.predict(valid_xs) for t in m.estimators_]) . r_mse(preds.mean(0), valid_y) . 0.232851 . plt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]); . Out-of-Bag Error . In the above training, you may have noticed that although the validation did well, it performed worse in comparison to the training. We can determine if this is a case of overfitting or something else by using OOB error. . The OOB error is a way of measuring prediction error on the training set by only including in the calculation of a row&#39;s error trees where that row was not included in training. This allows us to see whether the model is overfitting, without needing a separate validation set. . r_mse(m.oob_prediction_, y) . 0.210522 . Sidebar: Model Interpretation . For tabular data, model interpretation is particularly important. For a given model, the things we are most likely to be interested in are: . How confident are we in our predictions using a particular row of data? | For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? | Which columns are the strongest predictors, which can we ignore? | Which columns are effectively redundant with each other, for purposes of prediction? | How do predictions vary, as we vary these columns? | . Let&#39;s start with the first one! . Tree Variance for Prediction Confidence . preds = np.stack([t.predict(valid_xs) for t in m.estimators_]) #Grab all predictions . preds.shape . (40, 7988) . preds_std = preds.std(0) #Take standard dev across all (dim=0) . preds_std[:5] . array([0.29684344, 0.11715659, 0.09982913, 0.28225758, 0.10176538]) . It seems like the pred vary quite a lot. This can hint toward lower confidence . Feature Importance . def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) . fi = rf_feat_importance(m, xs) fi[:10] . cols imp . 59 YearMade | 0.177578 | . 31 Coupler_System | 0.128965 | . 7 ProductSize | 0.109147 | . 8 fiProductClassDesc | 0.080394 | . 33 Hydraulics_Flow | 0.061755 | . 56 ModelID | 0.058135 | . 51 saleElapsed | 0.051770 | . 4 fiSecondaryDesc | 0.043525 | . 2 fiModelDesc | 0.033589 | . 54 SalesID | 0.024917 | . These are the 10 most important features . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . Removing Low-Importance Variables . Lets retrain the model with only the important variables . to_keep = fi[fi.imp&gt;0.005].cols len(to_keep) . 21 . to_keep . 59 YearMade 31 Coupler_System 7 ProductSize 8 fiProductClassDesc 33 Hydraulics_Flow ... 24 Hydraulics 10 ProductGroup 12 Drive_System 29 Tire_Size 11 ProductGroupDesc Name: cols, Length: 21, dtype: object . xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] . m = rf(xs_imp, y) #fitting . m_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y) . (0.181614, 0.231854) . Notice same results, but with much fewer columns . len(xs.columns), len(xs_imp.columns) . (66, 21) . plot_fi(rf_feat_importance(m, xs_imp)); . Removing Redundant Features . cluster_columns(xs_imp) #This method creates clusters for us . def get_oob(df): m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15, max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True) m.fit(df, y) return m.oob_score_ . get_oob(xs_imp) . 0.8768766300713627 . {c:get_oob(xs_imp.drop(c, axis=1)) for c in ( &#39;saleYear&#39;, &#39;saleElapsed&#39;, &#39;ProductGroupDesc&#39;,&#39;ProductGroup&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;Hydraulics_Flow&#39;,&#39;Grouser_Tracks&#39;, &#39;Coupler_System&#39;)} . {&#39;saleYear&#39;: 0.8765064361622091, &#39;saleElapsed&#39;: 0.8723616014457026, &#39;ProductGroupDesc&#39;: 0.8766195232652162, &#39;ProductGroup&#39;: 0.8778533737393852, &#39;fiModelDesc&#39;: 0.8756466820705895, &#39;fiBaseModel&#39;: 0.8772457507706705, &#39;Hydraulics_Flow&#39;: 0.877162325587534, &#39;Grouser_Tracks&#39;: 0.8773995836616898, &#39;Coupler_System&#39;: 0.8781344943040315} . to_drop = [&#39;saleYear&#39;, &#39;ProductGroupDesc&#39;, &#39;fiBaseModel&#39;, &#39;Grouser_Tracks&#39;] get_oob(xs_imp.drop(to_drop, axis=1)) . 0.8749910789330729 . Notice that even after dropping 5 columns, we recieved very similer score . xs_final = xs_imp.drop(to_drop, axis=1) valid_xs_final = valid_xs_imp.drop(to_drop, axis=1) . save_pickle(Path()/&#39;xs_final.pkl&#39;, xs_final) save_pickle(Path()/&#39;valid_xs_final.pkl&#39;, valid_xs_final) . xs_final = load_pickle(Path()/&#39;xs_final.pkl&#39;) valid_xs_final = load_pickle(Path()/&#39;valid_xs_final.pkl&#39;) . m = rf(xs_final, y) m_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y) . (0.183687, 0.233937) . Partial Dependence . p = valid_xs_final[&#39;ProductSize&#39;].value_counts(sort=False).plot.barh() c = to.classes[&#39;ProductSize&#39;] plt.yticks(range(len(c)), c); . ax = valid_xs_final[&#39;YearMade&#39;].hist() . from sklearn.inspection import plot_partial_dependence fig,ax = plt.subplots(figsize=(12, 4)) plot_partial_dependence(m, valid_xs_final, [&#39;YearMade&#39;,&#39;ProductSize&#39;], grid_resolution=20, ax=ax); . Tree Interpreter . row = valid_xs_final.iloc[:5] . prediction,bias,contributions = treeinterpreter.predict(m, row.values) . prediction[0], bias[0], contributions[0].sum() . (array([9.97289227]), 10.104706206311942, -0.13181393330974048) . waterfall(valid_xs_final.columns, contributions[0], threshold=0.08, rotation_value=45,formatting=&#39;{:,.3f}&#39;); . End Sidebar . Sidebar: The Extrapolation Problem . x_lin = torch.linspace(0,20, steps=40) y_lin = x_lin + torch.randn_like(x_lin) plt.scatter(x_lin, y_lin); . xs_lin = x_lin.unsqueeze(1) #Must add another dim x_lin.shape,xs_lin.shape . (torch.Size([40]), torch.Size([40, 1])) . x_lin[:,None].shape #Another way . torch.Size([40, 1]) . m_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30]) #Train . plt.scatter(x_lin, y_lin, 20) plt.scatter(x_lin, m_lin.predict(xs_lin), color=&#39;red&#39;, alpha=0.5); . This is odd, why does our predictions look like that at the end? . Why? . Remember, a random forest just averages the predictions of a number of trees. And a tree simply predicts the average value of the rows in a leaf. Therefore, a tree and a random forest can never predict values outside of the range of the training data. That&#39;s why we need to make sure our validation set does not contain out-of-domain data. . Finding Out-of-Domain Data . We can find these out of domain data by doing the following: . df_dom = pd.concat([xs_final, valid_xs_final]) #Concat of training and valid is_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final)) #dependent var m = rf(df_dom, is_valid) rf_feat_importance(m, df_dom)[:6] . cols imp . 6 saleElapsed | 0.888724 | . 9 SalesID | 0.094268 | . 12 MachineID | 0.011024 | . 0 YearMade | 0.001854 | . 5 ModelID | 0.000571 | . 3 fiProductClassDesc | 0.000541 | . So these top 3 columns most different between the training and validation set . m = rf(xs_final, y) print(&#39;orig&#39;, m_rmse(m, valid_xs_final, valid_y)) for c in (&#39;SalesID&#39;,&#39;saleElapsed&#39;,&#39;MachineID&#39;): m = rf(xs_final.drop(c,axis=1), y) print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y)) . orig 0.231293 SalesID 0.230695 saleElapsed 0.235358 MachineID 0.230651 . Seems like we can remove the SalesID and MachineiD . time_vars = [&#39;SalesID&#39;,&#39;MachineID&#39;] xs_final_time = xs_final.drop(time_vars, axis=1) valid_xs_time = valid_xs_final.drop(time_vars, axis=1) m = rf(xs_final_time, y) m_rmse(m, valid_xs_time, valid_y) . 0.229106 . xs[&#39;saleYear&#39;].hist(); . Seems like most of the sales were after 2004, why don&#39;t we only look at that data . filt = xs[&#39;saleYear&#39;]&gt;2004 xs_filt = xs_final_time[filt] y_filt = y[filt] . m = rf(xs_filt, y_filt) m_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y) . (0.177593, 0.229799) . End Sidebar . Using a Neural Network . So far we have been using random forest and have been fitting it. We can also do this problem using a NN. Below will switch to using a NN. . Grabbing initial data . Like before, lets just grab everything we need . df_nn = pd.read_csv(Path()/&#39;TrainAndValid.csv&#39;, low_memory=False) #Data #Set ordinal variables using our order sizes = &#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39; df_nn[&#39;ProductSize&#39;] = df_nn[&#39;ProductSize&#39;].astype(&#39;category&#39;) df_nn[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) dep_var = &#39;SalePrice&#39; df_nn[dep_var] = np.log(df_nn[dep_var]) #remember we need to take log of the label (Kaggle requires) df_nn = add_datepart(df_nn, &#39;saledate&#39;) #Also remember that we used feature engineering on date . df_nn.shape #currently we have 65 col . (412698, 65) . df_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]] #lets drop the col down to the 15 we found earlier + the dep_var . df_nn_final.shape #now were down to our orig 16 col . (412698, 16) . cont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var) #Max_card makes it so that any col with more than # 9000 lvls, it will be treated as cont . cont_nn . [] . There&#39;s one variable that we absolutely do not want to treat as categorical:the saleElapsed variable . cont_nn.append(&#39;saleElapsed&#39;) #lets add that cont_nn . [&#39;saleElapsed&#39;] . cat_nn.remove(&#39;saleElapsed&#39;) #Remove from cat . Lets take a look at the carinality of each categorical data . df_nn_final[cat_nn].nunique() . YearMade 73 Coupler_System 2 ProductSize 6 fiProductClassDesc 74 Hydraulics_Flow 3 ModelID 5281 fiSecondaryDesc 177 fiModelDesc 5059 Enclosure 6 fiModelDescriptor 140 Hydraulics 12 ProductGroup 6 Drive_System 4 Tire_Size 17 dtype: int64 . Seems like fiModelDescriptor and fiModelDesc are similer, lets see if we can remove one them . xs_filt2 = xs_filt.drop(&#39;fiModelDescriptor&#39;, axis=1)#Drop on train valid_xs_time2 = valid_xs_time.drop(&#39;fiModelDescriptor&#39;, axis=1)#Drop on val m2 = rf(xs_filt2, y_filt) #create random forest m_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y) #test . (0.17662, 0.230076) . Seems like we can remove it! . cat_nn.remove(&#39;fiModelDescriptor&#39;) #remove . Creating dataloader . Like before we will create our TabularPandas Object. However, this time our procs has Normalize: This is added because NN are actually affected by the distribution of data due to their linear layers. . procs_nn = [Categorify, FillMissing, Normalize] to_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var) . If you get an error of &#39;Value Error:Unable to coerce to Series&#39;, try changing saleElapsed to int64 python df_nn_final.dtypes df_nn_final[&#39;saleElapsed&#39;] = df_nn_final[&#39;saleElapsed&#39;].astype(&#39;int&#39;) . dls = to_nn.dataloaders(1024) #lets create minibatchs of size 1024 . y = to_nn.train.y y.min(),y.max() . (8.465899, 11.863583) . Training . learn = tabular_learner(dls, y_range=(8,12), layers=[500,250], n_out=1, loss_func=F.mse_loss) . learn.lr_find() #find best lr . SuggestedLRs(lr_min=0.002754228748381138, lr_steep=0.00015848931798245758) . learn.fit_one_cycle(5, 1e-2) #train . epoch train_loss valid_loss time . 0 | 0.069279 | 0.061849 | 00:09 | . 1 | 0.056090 | 0.055828 | 00:09 | . 2 | 0.048482 | 0.055626 | 00:09 | . 3 | 0.043522 | 0.051739 | 00:09 | . 4 | 0.040515 | 0.051112 | 00:09 | . preds,targs = learn.get_preds() r_mse(preds,targs) . 0.22608 . Seems like it did better than the random forest fit . learn.save(&#39;nn&#39;) #save model . Ensembling . Both the random forest and NN have their benifits. Well we can actually get the best of both worlds by using an ensemble! . rf_preds = m2.predict(valid_xs_time2) #grab random forest pred ens_preds = (to_np(preds.squeeze()) + rf_preds) /2 #Take the average of our 2 pred . r_mse(ens_preds,valid_y) . 0.22251 . Compare this with the last result :) . Conclusion . Well I hoped you learn how to do tabular modeling using both a random forest and NN. They both have their own benefits. . Questionnaire . What is a continuous variable? A numerical value that is continous, such as age. | What is a categorical variable? An ordinal data, or data with discrete levels. | Provide two of the words that are used for the possible values of a categorical variable. ordinal variable and categorical variable. | What is a &quot;dense layer&quot;? Linear layers | How do entity embeddings reduce memory usage and speed up neural networks? Entity embeddings allows the indexing of data to be much more memory-efficient. | What kinds of datasets are entity embeddings especially useful for? Datasets with high levels of cardinality | What are the two main families of machine learning algorithms? Ensemble of decision trees are best for structured data (Ex tabular) Multilayered neural networks are best for unstructured data (Ex vision) | Why do some categorical columns need a special ordering in their classes? How do you do this in Pandas? These are ordinal data. To do this, we create our own order and pass it through. . sizes = &#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39; #our desired order df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].astype(&#39;category&#39;) #Turn into categorical variable df[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) #Now we can set our order . | Summarize what a decision tree algorithm does. Series of yes and no questions, which it used to determine how to group the data. Here is the algorithm given in the book: | Loop through each column of the dataset in turn | For each column, loop through each possible level of that column in turn | Try splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a - categorical variable, based on whether they are equal to or not equal to that level of that categorical variable) | Find the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple “model” where our predictions are simply the average sale price of the item’s group | After looping through all of the columns and possible levels for each, pick the split point which gave the best predictions using our very simple model | We now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each, by going back to step one for each group | Continue this process recursively, and until you have reached some stopping criterion for each group — for instance, stop splitting a group further when it has only 20 items in it. | . Why is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model? Dates are different from other cetegorical/continuous data (Ex: some are holidays). Therefore, we can generate many different categorical features about the given date (ex: Is it the end of the month?) | Should you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick? You should not because here the date plays a major part as the test set contains data of the last 2 weeks. Therefore, we should split the data by the dates and include the later dates in the validation set. | What is pickle and what is it useful for? Allows you so save any Python object as a file. | How are mse, samples, and values calculated in the decision tree drawn in this chapter? By traversing the tree (based on answering questions about the data), we reach the nodes that tell us the average value of the data in that group. | How do we deal with outliers, before building a decision tree? We can use random forest. But in this case we don’t use a random forest to predict our actual dependent variable. Instead we try to predict whether a row is in the validation set, or the training set. | How do we handle categorical variables in a decision tree? We convert it into a numerical value that references the vocab. | What is bagging? Training multiple models on random subsets of the data, and use the ensemble of models for prediction. | What is the difference between max_samples and max_features when creating a random forest? max_samples defines how many rows of the tabular dataset we use for each decision tree. max_features defines how many columns of the tabular dataset we use for each decision tree. | If you increase n_estimators to a very high value, can that lead to overfitting? Why or why not? No as the trees are independent of one another. | In the section &quot;Creating a Random Forest&quot;, just after &lt;&gt;, why did preds.mean(0) give the same result as our random forest?&lt;/strong&gt; Because much like the Random Forest that took the mean of the ensemble, we stacked all the dicision trees and took the mean across all the trees (dim=0).&lt;/li&gt; What is &quot;out-of-bag-error&quot;? The OOB error is a way of measuring prediction error on the training set by only including in the calculation of a row&#39;s error trees where that row was not included in training. This allows us to see whether the model is overfitting, without needing a separate validation set. | Make a list of reasons why a model&#39;s validation set error might be worse than the OOB error. How could you test your hypotheses? Overfitting Validation has different distribution | &lt;/ol&gt; One way you can solve the distribution problem is by checking the standard deviation on the predictions. . Explain why random forests are well suited to answering each of the following question: How confident are we in our predictions using a particular row of data? Check the standard deviation on the predictions. | For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? Check how the prediction changes as it goes through the tree, adding up the contributions from each split/feature. Use waterfall plot to visualize. | Which columns are the strongest predictors? This is done by using featureimportance | How do predictions vary as we vary these columns? Partial dependence plots | . | What&#39;s the purpose of removing unimportant variables? Improve model as its more interpertable with less clutering. Also, sometimes unnessary data can scew the prediction. | What&#39;s a good type of plot for showing tree interpreter results? Waterfall plot | What is the &quot;extrapolation problem&quot;? This was a demonstration of how a random forests cannot predict outside the domain of the training data. However, NN can generalize better due to their linear layers. | How can you tell if your test or validation set is distributed in a different way than your training set? We can do so by training a model to classify if the data is training or validation data. If the data is of different distributions (out-of-domain data), then the model can properly classify between the two datasets. | Why do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values? We want to make this a continuous variable as we want our model to determine future sales. | What is &quot;boosting&quot;? We train a model that underfits the dataset, and train subsequent models that predicts the error of the original model. We then add the predictions of all the models to get the final prediction. | How could we use embeddings with a random forest? Would we expect this to help? Instead of passing raw categorical columns, we can pass entity embeddings into the random forest model: This is better as embedding contain better representations of the features and will, in turn, improve the performance. | Why might we not always use a neural net for tabular modeling? We might not use them because they are harder and longer to train. | &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Further Research . Pick a competition on Kaggle with tabular data (current or past) and try to adapt the techniques seen in this chapter to get the best possible results. Compare your results to the private leaderboard. Completed, see https://usama280.github.io/PasteBlogs/ (Tabular on Lesson 8) | Implement the decision tree algorithm in this chapter from scratch yourself, and try it on the dataset you used in the first exercise. | Use the embeddings from the neural net in this chapter in a random forest, and see if you can improve on the random forest results we saw. Completed, see https://usama280.github.io/PasteBlogs/ (EmbeddingRandomForest) | Explain what each line of the source of TabularModel does (with the exception of the BatchNorm1d and Dropout layers). | &lt;/div&gt; |",
            "url": "https://usama280.github.io/PasteBlogs/2021/07/18/Lesson9-tabular.html",
            "relUrl": "/2021/07/18/Lesson9-tabular.html",
            "date": " • Jul 18, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Lesson 8 - FastAI",
            "content": "Collaborative Filtering Deep Dive . Collaborative filtering is a technique used by recommender systems. We will be taking a look at a movie reccomendation model. . A First Look at the Data . from fastai.collab import * from fastai.tabular.all import * path = untar_data(URLs.ML_100k) . ratings = pd.read_csv(path/&#39;u.data&#39;, delimiter=&#39; t&#39;, header=None, names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) ratings.head() . user movie rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . Lets simulate . Below we are simulating the reccomendation model. Here we assume know what the Latent Factors, but in reality we do not and need to determine them. . #Sci-fi, action, old last_skywalker = np.array([0.98,0.9,-0.9]) . user1 = np.array([0.9,0.8,-0.6]) . (user1*last_skywalker).sum() . 2.1420000000000003 . Pos val means that the user probably will like it . casablanca = np.array([-0.99,-0.3,0.8]) . (user1*casablanca).sum() . -1.611 . Neg val means that the user might not like it . movies = pd.read_csv(path/&#39;u.item&#39;, delimiter=&#39;|&#39;, encoding=&#39;latin-1&#39;, usecols=(0,1), names=(&#39;movie&#39;,&#39;title&#39;), header=None) movies.head() . movie title . 0 1 | Toy Story (1995) | . 1 2 | GoldenEye (1995) | . 2 3 | Four Rooms (1995) | . 3 4 | Get Shorty (1995) | . 4 5 | Copycat (1995) | . Lets merge our two tables . ratings = ratings.merge(movies) ratings.head() . user movie rating timestamp title . 0 196 | 242 | 3 | 881250949 | Kolya (1996) | . 1 63 | 242 | 3 | 875747190 | Kolya (1996) | . 2 226 | 242 | 5 | 883888671 | Kolya (1996) | . 3 154 | 242 | 3 | 879138235 | Kolya (1996) | . 4 306 | 242 | 5 | 876503793 | Kolya (1996) | . Creating dataloader . dls = CollabDataLoaders.from_df(ratings, user_name = &#39;user&#39;, item_name=&#39;title&#39;, bs=64) #must pass the correct columns dls.show_batch() . user title rating . 0 542 | My Left Foot (1989) | 4 | . 1 422 | Event Horizon (1997) | 3 | . 2 311 | African Queen, The (1951) | 4 | . 3 595 | Face/Off (1997) | 4 | . 4 617 | Evil Dead II (1987) | 1 | . 5 158 | Jurassic Park (1993) | 5 | . 6 836 | Chasing Amy (1997) | 3 | . 7 474 | Emma (1996) | 3 | . 8 466 | Jackie Chan&#39;s First Strike (1996) | 3 | . 9 554 | Scream (1996) | 3 | . dls.classes #We have the user and title class len(dls.classes[&#39;user&#39;]), . (944,) . Initialize Parameters . n_users = len(dls.classes[&#39;user&#39;]) n_movies = len(dls.classes[&#39;title&#39;]) n_factors = 5 #Number of latent factors user_factors = torch.randn(n_users, n_factors) movie_factors = torch.randn(n_movies, n_factors) . Sidebar: Indexing . It turns out we can represent looking up an index as a matrix. See below . one_hot_3 = one_hot(3, n_users).float() one_hot_3[:10] . tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]) . user_factors[3] #parameter (latent factors) values at this index are . tensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908]) . user_factors.t() @ one_hot_3 . tensor([ 0.4286, 0.8374, -0.5413, -1.6935, 0.1618]) . Notice same values . End Sidebar . Collaborative Filtering from Scratch . Lets put what we did above into a class. This class will initialize the parameters for us and forward pass as well. . class DotProduct(Module): #extends Module class def __init__(self, n_users, n_movies, n_factors): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) def forward(self, x): #Method called auto. anytime using Module class users = self.user_factors(x[:,0]) #user ID&#39;s movies = self.movie_factors(x[:,1]) #movie ID&#39;s return (users * movies).sum(dim=1) #dim=0 is the minibatches, we want to sum over the other dim (1) . x,y = dls.one_batch() x.shape . torch.Size([64, 2]) . x[:3] #user ID, movie ID . tensor([[655, 256], [298, 329], [862, 185]], device=&#39;cuda:0&#39;) . y[:3] #ratings . tensor([[5], [4], [4]], device=&#39;cuda:0&#39;, dtype=torch.int8) . Training . model = DotProduct(n_users, n_movies, 50) #our model learn = Learner(dls, model, loss_func=MSELossFlat()) . learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 1.325196 | 1.266774 | 00:11 | . 1 | 1.029445 | 1.041569 | 00:11 | . 2 | 0.941498 | 0.949599 | 00:11 | . 3 | 0.823250 | 0.879281 | 00:11 | . 4 | 0.758273 | 0.859028 | 00:10 | . Not bad, but we can do better! . Improving the model . We can improve our model by giving it a range as its a regression model. Passing it the range of (0-5.5) should improve the performance. . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return sigmoid_range((users * movies).sum(dim=1), *self.y_range) . model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 1.004179 | 0.974668 | 00:10 | . 1 | 0.874107 | 0.902177 | 00:11 | . 2 | 0.700155 | 0.859749 | 00:10 | . 3 | 0.500587 | 0.870938 | 00:10 | . 4 | 0.378323 | 0.876955 | 00:10 | . Didn&#39;t really improve, but thats ok . Further improving the model . We should also add a bias as some rating may be skewing the data. . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) self.user_bias = Embedding(n_users, 1) self.movie_bias = Embedding(n_movies, 1) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) res = (users * movies).sum(dim=1, keepdim=True) #bias res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) return sigmoid_range(res, *self.y_range) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.956606 | 0.929338 | 00:13 | . 1 | 0.799096 | 0.850351 | 00:13 | . 2 | 0.607060 | 0.845230 | 00:15 | . 3 | 0.403307 | 0.869548 | 00:14 | . 4 | 0.289079 | 0.877028 | 00:14 | . Loss not improving . It seems like our loss is not improving regardless of the improvements: But if you take another look you should realise that it performs better during the earlier epochs (2 or 3). This means that we are overfitting the model. But how can we train it for more epochs without overfitting? This is where weight regularization comes in. . Weight Decay . Weight decay is a regularization technique that penalizes the model for large weights. Overall, it prevents the model from overfitting during training. . x = np.linspace(-2,2,100) a_s = [1,2,5,10,50] ys = [a * x**2 for a in a_s] _,ax = plt.subplots(figsize=(8,6)) for a,y in zip(a_s,ys): ax.plot(x,y, label=f&#39;a={a}&#39;) ax.set_ylim([0,5]) ax.legend(); . Graphic above demonstrates weight decay . Train with weight decay . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) #Pass wd . epoch train_loss valid_loss time . 0 | 0.959248 | 0.956658 | 00:10 | . 1 | 0.870590 | 0.876975 | 00:09 | . 2 | 0.738598 | 0.837762 | 00:09 | . 3 | 0.593487 | 0.822684 | 00:10 | . 4 | 0.483328 | 0.823074 | 00:09 | . Nice our loss dropped down to .82! Also, notice that train_loss increased, this is because wd is preventing the model from overfitting . Using FastAI ToolKit . Can also achieve same results using FastAI ToolKit. Notice we changed from Learner to collab_learner. . learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) . learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.931780 | 0.960767 | 00:10 | . 1 | 0.881490 | 0.876014 | 00:09 | . 2 | 0.762739 | 0.831577 | 00:09 | . 3 | 0.590236 | 0.823882 | 00:09 | . 4 | 0.493440 | 0.824700 | 00:09 | . movie_bias = learn.model.i_bias.weight.squeeze() idxs = movie_bias.argsort(descending=True)[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . Sidebar: Creating Our Own Embedding Module . So far we have been using the predefined Embedding, why don&#39;t we create our own? . class T(Module): def __init__(self): self.a = torch.ones(3) L(T().parameters())# calling method from Module class . (#0) [] . class T(Module): def __init__(self): self.a = nn.Parameter(torch.ones(3)) #Must wrap it with nn.Parameter() L(T().parameters()) . (#1) [Parameter containing: tensor([1., 1., 1.], requires_grad=True)] . class T(Module): def __init__(self): self.a = nn.Linear(1, 3, bias=False) t = T() L(t.parameters()) . (#1) [Parameter containing: tensor([[-0.4927], [ 0.4325], [ 0.5283]], requires_grad=True)] . def create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) . This is all we need to create our own embedding . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.movie_factors = create_params([n_movies, n_factors]) self.movie_bias = create_params([n_movies]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] movies = self.movie_factors[x[:,1]] res = (users*movies).sum(dim=1) res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] return sigmoid_range(res, *self.y_range) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.960358 | 0.956795 | 00:10 | . 1 | 0.869042 | 0.874685 | 00:10 | . 2 | 0.737840 | 0.839419 | 00:10 | . 3 | 0.589841 | 0.823726 | 00:10 | . 4 | 0.472334 | 0.824282 | 00:10 | . Notice similer performance . End Sidebar . Looking inside model . We can take a look inside our model by calling learn.model. . movie_bias = learn.model.movie_bias.squeeze() #Grab movie by bias idxs = movie_bias.argsort()[:5] #Sort by least bias [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#39;Children of the Corn: The Gathering (1996)&#39;, &#39;Robocop 3 (1993)&#39;, &#39;Lawnmower Man 2: Beyond Cyberspace (1996)&#39;, &#39;Amityville 3-D (1983)&#39;, &#39;Mortal Kombat: Annihilation (1997)&#39;] . Least bias movies . idxs = movie_bias.argsort(descending=True)[:5] #Sort by most bias [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#39;Titanic (1997)&#39;, &#39;L.A. Confidential (1997)&#39;, &#39;Silence of the Lambs, The (1991)&#39;, &#39;Shawshank Redemption, The (1994)&#39;, &#39;Star Wars (1977)&#39;] . Most bias movies . g = ratings.groupby(&#39;title&#39;)[&#39;rating&#39;].count() top_movies = g.sort_values(ascending=False).index.values[:1000] top_idxs = tensor([learn.dls.classes[&#39;title&#39;].o2i[m] for m in top_movies]) movie_w = learn.model.movie_factors[top_idxs].cpu().detach() movie_pca = movie_w.pca(3) fac0,fac1,fac2 = movie_pca.t() idxs = list(range(50)) X = fac0[idxs] Y = fac2[idxs] plt.figure(figsize=(12,12)) plt.scatter(X, Y) for i, x, y in zip(top_movies[idxs], X, Y): plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11) plt.show() . Notice that similer movies have been clumped togather . Embedding Distance . We can also use simple math to find similer movies . movie_factors = learn.model.movie_factors idx = dls.classes[&#39;title&#39;].o2i[&#39;Forrest Gump (1994)&#39;] distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None]) idx = distances.argsort(descending=True)[1] dls.classes[&#39;title&#39;][idx] . &#39;Affair to Remember, An (1957)&#39; . Sidebar: Bootstrapping a Collaborative Filtering Model . Another approach to a Collaborative Filtering Model . embs = get_emb_sz(dls) embs . [(944, 74), (1665, 102)] . class CollabNN(Module): def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+item_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim=1)) return sigmoid_range(x, *self.y_range) . model = CollabNN(*embs) . learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) . epoch train_loss valid_loss time . 0 | 0.919071 | 0.944800 | 00:11 | . 1 | 0.920309 | 0.907606 | 00:10 | . 2 | 0.844579 | 0.880101 | 00:10 | . 3 | 0.810155 | 0.865898 | 00:10 | . 4 | 0.746803 | 0.869486 | 00:10 | . Using FastAI ToolKit . Can also achieve same results using FastAI ToolKit. Just enable use_nn . learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.966049 | 0.983382 | 00:13 | . 1 | 0.891749 | 0.926404 | 00:13 | . 2 | 0.863655 | 0.885933 | 00:13 | . 3 | 0.825138 | 0.864230 | 00:13 | . 4 | 0.740462 | 0.860209 | 00:13 | . Notice similer results . type(learn.model) . fastai.collab.EmbeddingNN . @delegates(TabularModel) class EmbeddingNN(TabularModel): def __init__(self, emb_szs, layers, **kwargs): super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs) . End sidebar . Conclusion . Overall, I hope you learned how to create a reccomendation model. Some very important concepts were viewed at, such as latent factors and weight decay. . Questionnaire . What problem does collaborative filtering solve? Obtains latent factors needed to provided a good reccomendation. | How does it solve it? It learns the latent factors via gradient descent and clumps up similer kind of factors. | Why might a collaborative filtering predictive model fail to be a very useful recommendation system? If there is a lack of data from users to provide useful reccomendations. | What does a crosstab representation of collaborative filtering data look like? Crosstab is where the colomn tabs are users, the row tabs are items, and values are filled out based on the user’s rating of the items. | Write the code to create a crosstab representation of the MovieLens data (you might need to do some web searching!). . | What is a latent factor? Why is it &quot;latent&quot;? Latent factos are the factors used to determine a prediction. They are latent as they are learned, NOT given to the model. . | What is a dot product? Calculate a dot product manually using pure Python with lists. The dot product is the sum of the products of the corresponding matrixs. . a = [1,2,3] b = [1,2,3] sum(i[0]*i[1] for i in zip(a,b)) . | What does pandas.DataFrame.merge do? Merges two DataFrame&#39;s togather | What is an embedding matrix? It is what you multiply an embedding with. | What is the relationship between an embedding and a matrix of one-hot-encoded vectors? The embedding is a matrix of a one-hot encoded vecotors, but is more computationally efficient. | Why do we need Embedding if we could use one-hot-encoded vectors for the same thing? More computationally efficient and fastser. | What does an embedding contain before we start training (assuming we&#39;re not using a pretained model)? It is randomly initialized. | Create a class (without peeking, if possible!) and use it. . class Name: def __init__def(self): pass def func_name(self): pass . | What does x[:,0] return? User ids | Rewrite the DotProduct class (without peeking, if possible!) and train a model with it. . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return sigmoid_range((users * movies).sum(dim=1), *self.y_range) . | What is a good loss function to use for MovieLens? Why? Mean Squared Error Loss. Can use to compare how far the prediction is from the label. | What would happen if we used cross-entropy loss with MovieLens? How would we need to change the model? We would need the model to output more predictions, only then can we pass it to the cross-entropy. | What is the use of bias in a dot product model? Some rating may skeew the data, so a bias can help. | What is another name for weight decay? L2 regularization | Write the equation for weight decay (without peeking!). loss_with_wd = loss + wd * (parameters**2).sum() | Write the equation for the gradient of weight decay. Why does it help reduce weights? Add to the gradients 2wdparameters. Overall, this prevents overfitting by enforcing more evenly distributed weights. | Why does reducing weights lead to better generalization? By enforcing more evenly distributed weights, the result is is more shallow, with less sharp surfaces. | What does argsort do in PyTorch? Returns index values of the order of the original list. | Does sorting the movie biases give the same result as averaging overall movie ratings by movie? Why/why not? No. It takes into account other factors which can influence the results. | How do you print the names and details of the layers in a model? learn.model | What is the &quot;bootstrapping problem&quot; in collaborative filtering? The model cannot make any recommendations without enough data | How could you deal with the bootstrapping problem for new users? For new movies? Have them complete a questionair. | How can feedback loops impact collaborative filtering systems? May cause the model to suffer from bias. | When using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users? Because we are not taking the dot product, and instead concatenating the embedding matrices, different number of factors is alright. | Why is there an nn.Sequential in the CollabNN model? Can create nonlinearity as we can couple multiple layers together. | What kind of model should we use if we want to add metadata about users and items, or information such as date and time, to a collaborative filtering model? Tabular model | Further Research . Take a look at all the differences between the Embedding version of DotProductBias and the create_params version, and try to understand why each of those changes is required. If you&#39;re not sure, try reverting each change to see what happens. (NB: even the type of brackets used in forward has changed!) . | Find three other areas where collaborative filtering is being used, and find out what the pros and cons of this approach are in those areas. . | Complete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the book&#39;s website and the fast.ai forum for ideas. Note that there are more columns in the full dataset—see if you can use those too (the next chapter might give you ideas). . | Create a model for MovieLens that works with cross-entropy loss, and compare it to the model in this chapter. Completed, see here https://usama280.github.io/PasteBlogs/ . |",
            "url": "https://usama280.github.io/PasteBlogs/2021/07/15/Lesson8-collab.html",
            "relUrl": "/2021/07/15/Lesson8-collab.html",
            "date": " • Jul 15, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Lesson 6 - FastAI",
            "content": "Multi-Label Classification . So far we have been doing classfication for various images, but often pictures have more than one object present within them. For this reason, we will now cover multi-label classification. Our goal is to create a model that can classify all objects within an image. . The Data . path = untar_data(URLs.PASCAL_2007) Path.BASE_PATH = path path.ls() . (#8) [Path(&#39;train&#39;),Path(&#39;test.json&#39;),Path(&#39;segmentation&#39;),Path(&#39;train.json&#39;),Path(&#39;valid.json&#39;),Path(&#39;test.csv&#39;),Path(&#39;train.csv&#39;),Path(&#39;test&#39;)] . df = pd.read_csv(path/&#39;train.csv&#39;) df.head() . fname labels is_valid . 0 000005.jpg | chair | True | . 1 000007.jpg | car | True | . 2 000009.jpg | horse person | True | . 3 000012.jpg | car | False | . 4 000016.jpg | bicycle | True | . Sidebar: Pandas (pd) and DataFrames . df.iloc[:,0] #column 0 . 0 000005.jpg 1 000007.jpg 2 000009.jpg 3 000012.jpg 4 000016.jpg ... 5006 009954.jpg 5007 009955.jpg 5008 009958.jpg 5009 009959.jpg 5010 009961.jpg Name: fname, Length: 5011, dtype: object . df.iloc[0,:] #row 0 # Trailing :s are always optional (in numpy, pytorch, pandas, etc.), # so this is equivalent: df.iloc[0] . fname 000005.jpg labels chair is_valid True Name: 0, dtype: object . df[&#39;fname&#39;] . 0 000005.jpg 1 000007.jpg 2 000009.jpg 3 000012.jpg 4 000016.jpg ... 5006 009954.jpg 5007 009955.jpg 5008 009958.jpg 5009 009959.jpg 5010 009961.jpg Name: fname, Length: 5011, dtype: object . tmp_df = pd.DataFrame({&#39;a&#39;:[1,2], &#39;b&#39;:[3,4]}) tmp_df . a b . 0 1 | 3 | . 1 2 | 4 | . tmp_df[&#39;c&#39;] = tmp_df[&#39;a&#39;]+tmp_df[&#39;b&#39;] tmp_df . a b c . 0 1 | 3 | 4 | . 1 2 | 4 | 6 | . Exploring Dataloader and Datasets . a = list(enumerate(string.ascii_lowercase)) a[0], len(a) . ((0, &#39;a&#39;), 26) . dl_a = DataLoader(a, batch_size=8, shuffle=True) first(dl_a) . (tensor([17, 18, 10, 22, 8, 14, 20, 15]), (&#39;r&#39;, &#39;s&#39;, &#39;k&#39;, &#39;w&#39;, &#39;i&#39;, &#39;o&#39;, &#39;u&#39;, &#39;p&#39;)) . a = list((string.ascii_lowercase)) dss = Datasets(a) dss[0] . (&#39;a&#39;,) . End Sidebar . Constructing a DataBlock . You may have noticed, we often use a datablock when processing data. Why don&#39;t we create our own datablock from scratch? . dblock = DataBlock() . dsets = dblock.datasets(df) #This creates a train and valid set . len(dsets.train),len(dsets.valid) . (4009, 1002) . x,y = dsets.train[0] x,y . (fname 004719.jpg labels bus car is_valid True Name: 2369, dtype: object, fname 004719.jpg labels bus car is_valid True Name: 2369, dtype: object) . Notice that the x and y are identical! This means we need to create our own independent and dependent var . Must create our own independent and dependent var . x[&#39;fname&#39;] #This is our independent var . &#39;004719.jpg&#39; . y[&#39;labels&#39;] #This is our dependent var . &#39;bus car&#39; . dblock = DataBlock(get_x = lambda r: r[&#39;fname&#39;], get_y = lambda r: r[&#39;labels&#39;]) dsets = dblock.datasets(df) dsets.train[0] #Awesome it works, just one more thing, we need the path . (&#39;004849.jpg&#39;, &#39;train&#39;) . def get_x(r): return r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;] dblock = DataBlock( get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (&#39;004069.jpg&#39;, &#39;bird&#39;) . This is good but we need the path and need to index the labels . def get_x(r): return path/&#39;train&#39;/r[&#39;fname&#39;] #Need path def get_y(r): return r[&#39;labels&#39;].split(&#39; &#39;) #Split into each index dblock = DataBlock( get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] #looks good . (Path(&#39;train/003973.jpg&#39;), [&#39;bus&#39;, &#39;car&#39;, &#39;person&#39;]) . Looks good . Lets add the respective blocks (ImageBlock, MultiCategoryBlock) . To actually open the image and do the conversion to tensors, we will need to use blocks. Our independent block is obviously the images so ImageBlock. Our dependent block must be vary of more than one category so MultiCategoryBlock. . def get_x(r): return path/&#39;train&#39;/r[&#39;fname&#39;] #Need path def get_y(r): return r[&#39;labels&#39;].split(&#39; &#39;) #Split into each index dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), #Adding blocks get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) x,y = dsets.train[0] . Viewing data . x . y . TensorMultiCategory([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) . dsets.vocab . [&#39;aeroplane&#39;, &#39;bicycle&#39;, &#39;bird&#39;, &#39;boat&#39;, &#39;bottle&#39;, &#39;bus&#39;, &#39;car&#39;, &#39;cat&#39;, &#39;chair&#39;, &#39;cow&#39;, &#39;diningtable&#39;, &#39;dog&#39;, &#39;horse&#39;, &#39;motorbike&#39;, &#39;person&#39;, &#39;pottedplant&#39;, &#39;sheep&#39;, &#39;sofa&#39;, &#39;train&#39;, &#39;tvmonitor&#39;] . dsets.train[0][1] # Also known as y (Stored above) . TensorMultiCategory([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) . idxs = torch.where(dsets.train[0][1]==1)[0] dsets.vocab[idxs] . (#1) [&#39;bird&#39;] . Now lets split the data into training and valid . def splitter(df): train = df.index[~df[&#39;is_valid&#39;]].tolist() # ~ means NOT valid = df.index[df[&#39;is_valid&#39;]].tolist() return train,valid dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) . Finally, lets switch the dataset out for a dataloader . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y, item_tfms = RandomResizedCrop(128, min_scale=0.35)) #Must make all images same size dls = dblock.dataloaders(df) #Switching to dataloaders . dls.show_batch(nrows=1, ncols=3) . Loss function for multi-label (Binary Cross-Entropy) . The loss function, as you may have expected, has changed once again. Because we are dealing with multiple labels, we need to use binary cross entropy. . learn = cnn_learner(dls, resnet18) . x,y = to_cpu(dls.train.one_batch()) activs = learn.model(x) #Can view activations on a single batch activs.shape #64 imgs with 20 actv each . torch.Size([64, 20]) . activs[0] . TensorImage([ 2.1040, 3.5137, -1.8048, 4.0140, -2.7668, 5.4573, 0.4210, -2.2953, 2.8784, -0.6523, 1.9214, -2.0250, -0.2120, -1.9046, -1.1947, -0.1867, -0.9368, 0.4383, -1.1167, -0.8710], grad_fn=&lt;AliasBackward&gt;) . def binary_cross_entropy(inputs, targets): inputs = inputs.sigmoid() return -torch.where(targets==1, inputs, 1-inputs).log().mean() . loss_func = nn.BCEWithLogitsLoss() #Same thing as the func above loss = loss_func(activs, y) loss . TensorImage(1.0545, grad_fn=&lt;AliasBackward&gt;) . Accuracy . One change compared to the last chapter is the metric we use: because this is a multilabel problem, we can&#39;t use the accuracy function. Why is that? Well, accuracy was comparing our outputs to our targets like so: . def accuracy(inp, targ, axis=-1): &quot;Compute accuracy with `targ` when `pred` is bs * n_classes&quot; pred = inp.argmax(dim=axis) return (pred == targ).float().mean() . The class predicted was the one with the highest activation (this is what argmax does). Here it doesn&#39;t work because we could have more than one prediction on a single image. After applying the sigmoid to our activations (to make them between 0 and 1), we need to decide which ones are 0s and which ones are 1s by picking a threshold. Each value above the threshold will be considered as a 1, and each value lower than the threshold will be considered a 0: . def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True): &quot;Compute accuracy when `inp` and `targ` are the same size.&quot; if sigmoid: inp = inp.sigmoid() return ((inp&gt;thresh)==targ.bool()).float().mean() . Sidebar: Partial example . Showcasing how partial works in python . def say_hello(name, say_what=&quot;Hello&quot;): return f&quot;{say_what} {name}.&quot; say_hello(&#39;Jeremy&#39;),say_hello(&#39;Jeremy&#39;, &#39;Ahoy!&#39;) . (&#39;Hello Jeremy.&#39;, &#39;Ahoy! Jeremy.&#39;) . f = partial(say_hello, say_what=&quot;Bonjour&quot;) f(&quot;Jeremy&quot;),f(&quot;Sylvain&quot;) . (&#39;Bonjour Jeremy.&#39;, &#39;Bonjour Sylvain.&#39;) . End Sidebar . Training . learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2)) learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.950922 | 0.694729 | 0.243845 | 00:31 | . 1 | 0.830578 | 0.568214 | 0.294123 | 00:26 | . 2 | 0.609650 | 0.201799 | 0.818108 | 00:26 | . 3 | 0.364655 | 0.126312 | 0.939741 | 00:26 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.135176 | 0.116987 | 0.942052 | 00:32 | . 1 | 0.115532 | 0.106409 | 0.954303 | 00:32 | . 2 | 0.096683 | 0.103882 | 0.950458 | 00:32 | . 95% acc with a treshold of .2, lets try some more . learn.metrics = partial(accuracy_multi, thresh=0.1) #Try different thresh learn.validate() . (#2) [0.1038823127746582,0.9292030334472656] . learn.metrics = partial(accuracy_multi, thresh=0.99) learn.validate() . (#2) [0.1038823127746582,0.9444024562835693] . preds,targs = learn.get_preds() . accuracy_multi(preds, targs, thresh=0.9, sigmoid=False) . TensorImage(0.9584) . xs = torch.linspace(0.05,0.95,29) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . Next model - Regression . Now that we have taken a look at multi label model, lets take a look at a regression model! . Assemble the Data . path = untar_data(URLs.BIWI_HEAD_POSE) . path.ls().sorted() . (#50) [Path(&#39;01&#39;),Path(&#39;01.obj&#39;),Path(&#39;02&#39;),Path(&#39;02.obj&#39;),Path(&#39;03&#39;),Path(&#39;03.obj&#39;),Path(&#39;04&#39;),Path(&#39;04.obj&#39;),Path(&#39;05&#39;),Path(&#39;05.obj&#39;)...] . (path/&#39;01&#39;).ls().sorted() . (#1000) [Path(&#39;01/depth.cal&#39;),Path(&#39;01/frame_00003_pose.txt&#39;),Path(&#39;01/frame_00003_rgb.jpg&#39;),Path(&#39;01/frame_00004_pose.txt&#39;),Path(&#39;01/frame_00004_rgb.jpg&#39;),Path(&#39;01/frame_00005_pose.txt&#39;),Path(&#39;01/frame_00005_rgb.jpg&#39;),Path(&#39;01/frame_00006_pose.txt&#39;),Path(&#39;01/frame_00006_rgb.jpg&#39;),Path(&#39;01/frame_00007_pose.txt&#39;)...] . img_files = get_image_files(path) #jpg&#39;s im = PILImage.create(img_files[0]) im.shape . (480, 640) . im.to_thumb(160) . def img2pose(x): return Path(f&#39;{str(x)[:-7]}pose.txt&#39;) img2pose(img_files[0]) . Path(&#39;06/frame_00554_pose.txt&#39;) . cal = np.genfromtxt(path/&#39;01&#39;/&#39;rgb.cal&#39;, skip_footer=6) #Given func to get center of face def get_ctr(f): ctr = np.genfromtxt(img2pose(f), skip_header=3) c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2] c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2] return tensor([c1,c2]) . get_ctr(img_files[0]) . tensor([330.9554, 308.3703]) . Required datablock . biwi = DataBlock( blocks=(ImageBlock, PointBlock), get_items=get_image_files, get_y=get_ctr, splitter=FuncSplitter(lambda o: o.parent.name==&#39;13&#39;), #Just grabbing person number 13 as our valid set batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)] ) . dls = biwi.dataloaders(path) dls.show_batch(max_n=9, figsize=(8,6)) . xb,yb = dls.one_batch() xb.shape,yb.shape . (torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2])) . yb[0] . TensorPoint([[0.1999, 0.0785]], device=&#39;cuda:0&#39;) . Training a Model . learn = cnn_learner(dls, resnet18, y_range=(-1,1)) #range states what values are within the acceptable range # -1 is far-left and bottom, 1 is far-right and top . def sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo . plot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4) . /opt/conda/envs/fastai/lib/python3.8/site-packages/fastbook/__init__.py:73: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1603729096996/work/aten/src/ATen/native/RangeFactories.cpp:23.) x = torch.linspace(min,max) . dls.loss_func . FlattenedLoss of MSELoss() . learn.lr_find() . SuggestedLRs(lr_min=0.005754399299621582, lr_steep=1.5848931980144698e-06) . lr = 1e-2 learn.fine_tune(3, lr) . epoch train_loss valid_loss time . 0 | 0.049630 | 0.005448 | 03:34 | . epoch train_loss valid_loss time . 0 | 0.008197 | 0.002675 | 04:41 | . 1 | 0.003693 | 0.000182 | 04:40 | . 2 | 0.001383 | 0.000099 | 04:41 | . math.sqrt(0.0001) #error . 0.01 . learn.show_results(ds_idx=1, nrows=3, figsize=(6,8)) . Conclusion . Today you learned how to create both a multi-label and regression model. You may have noticed, when creating these models we often take similer approaches with minor differences. This is good because the similer the approach the easier it is to learn these distinct models. . Questionnaire . How could multi-label classification improve the usability of the bear classifier? It would be able to classify more bears in the image. Also, it would allow for the classification of no bears present. | How do we encode the dependent variable in a multi-label classification problem? It is one-hot encoded: Here the vector is the same length as the vocab. 0 and 1 are used to repersent if a class if present. | How do you access the rows and columns of a DataFrame as if it was a matrix? df.iloc[0][1] - row 0, col 1 | How do you get a column by name from a DataFrame? df[&#39;colName&#39;] | What is the difference between a Dataset and DataLoader? Dataset returns tuples of x, y DataLoader is an extention of Dataset, here it return minibatches of x,y | What does a Datasets object normally contain? Training and validation set | What does a DataLoaders object normally contain? Training dataloader and validation dataloader | What does lambda do in Python? Lambda is an anonymous function that can be created on the spot. Do note that they are not serializable, however. | What are the methods to customize how the independent and dependent variables are created with the data block API? get_x get_y | Why is softmax not an appropriate output activation function when using a one hot encoded target? Softmax forces model to pick only one class | Why is nll_loss not an appropriate loss function when using a one-hot-encoded target? Similer to Softmax, this works better when you want only one class | What is the difference between nn.BCELoss and nn.BCEWithLogitsLoss? BCELoss assumes you did sigmoid prior BCEWithLogitsLoss does sigmoid | Why can&#39;t we use regular accuracy in a multi-label problem? Regular accuracy assumes that only 1 class is correct. However, in multi-label problems there can be multiple labels, so a threshold is assigned. | When is it okay to tune a hyperparameter on the validation set? When the hyper-parameter and the metric being observed is smooth | How is y_range implemented in fastai? (See if you can implement it yourself and test it without peeking!) def sigmoid_range(x,lo, hi): return x.sigmoid() * (hi-lo) + lo . | What is a regression problem? What loss function should you use for such a problem? Dependent values are continuous. The loss functions used often is mean squared error loss. | What do you need to do to make sure the fastai library applies the same data augmentation to your inputs images and your target point coordinates? You must used the correct DataBlock, PointBlock. | Further Research . Read a tutorial about Pandas DataFrames and experiment with a few methods that look interesting to you. See the book&#39;s website for recommended tutorials. | Retrain the bear classifier using multi-label classification. See if you can make it work effectively with images that don&#39;t contain any bears, including showing that information in the web application. Try an image with two different kinds of bears. Check whether the accuracy on the single-label dataset is impacted using multi-label classification. Completed, see here [https://usama280.github.io/PasteBlogs/] |",
            "url": "https://usama280.github.io/PasteBlogs/2021/07/13/Lesson6-multicat.html",
            "relUrl": "/2021/07/13/Lesson6-multicat.html",
            "date": " • Jul 13, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Lesson 5 - FastAI",
            "content": "Image Classification . Lets create a classification model that can differentiate cat and dog breeds. And, lets improve this model&#39;s performance using some techniques! . from fastai.vision.all import * path = untar_data(URLs.PETS) . path.ls() . (#2) [Path(&#39;images&#39;),Path(&#39;annotations&#39;)] . (path/&quot;images&quot;).ls() . (#7393) [Path(&#39;images/miniature_pinscher_199.jpg&#39;),Path(&#39;images/newfoundland_183.jpg&#39;),Path(&#39;images/pomeranian_90.jpg&#39;),Path(&#39;images/pomeranian_102.jpg&#39;),Path(&#39;images/japanese_chin_74.jpg&#39;),Path(&#39;images/yorkshire_terrier_45.jpg&#39;),Path(&#39;images/chihuahua_34.jpg&#39;),Path(&#39;images/american_pit_bull_terrier_150.jpg&#39;),Path(&#39;images/wheaten_terrier_160.jpg&#39;),Path(&#39;images/staffordshire_bull_terrier_91.jpg&#39;)...] . fname = (path/&quot;images&quot;).ls()[0] fname . Path(&#39;images/miniature_pinscher_199.jpg&#39;) . Using regex to obtain label . Regex is a very handy tool to learn to extract patterns from strings. See below: . re.findall(r&#39;(.+)_ d+.jpg$&#39;, fname.name) . [&#39;miniature_pinscher&#39;] . Nice now we can pass these as the labels . pets = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), #split randomly get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), #getting labels using regex item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) #augmentation on data dls = pets.dataloaders(path/&quot;images&quot;) . dls.show_batch(nrows = 1, ncols=3) . dls.show_batch(nrows=1, ncols = 3, unique = True) . Lets test the model with what we have currently . learn = cnn_learner(dls, resnet18, metrics=accuracy) #Notice we didn&#39;t choose a loss, FastAI picks one for us learn.fit_one_cycle(2) . epoch train_loss valid_loss accuracy time . 0 | 1.532583 | 0.412520 | 0.868065 | 01:20 | . 1 | 0.705810 | 0.353400 | 0.887010 | 01:07 | . learn.loss_func #fastAI chose CrossEntropyLoss as the loss func . FlattenedLoss of CrossEntropyLoss() . Cross-Entropy Loss . What is cross-entropy loss? This is the loss function we use when classifying multiple objects. . x,y = dls.one_batch() . y #values refer to vocab list . TensorCategory([18, 19, 20, 23, 24, 35, 27, 27, 32, 16, 0, 9, 21, 0, 2, 12, 26, 10, 24, 20, 32, 27, 18, 28, 8, 18, 23, 21, 30, 29, 9, 26, 25, 29, 14, 11, 34, 7, 36, 4, 9, 6, 23, 20, 17, 14, 25, 19, 23, 8, 18, 2, 11, 7, 9, 19, 19, 31, 1, 29, 1, 31, 7, 33], device=&#39;cuda:0&#39;) . dls.vocab . [&#39;Abyssinian&#39;, &#39;Bengal&#39;, &#39;Birman&#39;, &#39;Bombay&#39;, &#39;British_Shorthair&#39;, &#39;Egyptian_Mau&#39;, &#39;Maine_Coon&#39;, &#39;Persian&#39;, &#39;Ragdoll&#39;, &#39;Russian_Blue&#39;, &#39;Siamese&#39;, &#39;Sphynx&#39;, &#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, &#39;basset_hound&#39;, &#39;beagle&#39;, &#39;boxer&#39;, &#39;chihuahua&#39;, &#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, &#39;german_shorthaired&#39;, &#39;great_pyrenees&#39;, &#39;havanese&#39;, &#39;japanese_chin&#39;, &#39;keeshond&#39;, &#39;leonberger&#39;, &#39;miniature_pinscher&#39;, &#39;newfoundland&#39;, &#39;pomeranian&#39;, &#39;pug&#39;, &#39;saint_bernard&#39;, &#39;samoyed&#39;, &#39;scottish_terrier&#39;, &#39;shiba_inu&#39;, &#39;staffordshire_bull_terrier&#39;, &#39;wheaten_terrier&#39;, &#39;yorkshire_terrier&#39;] . preds,_ = learn.get_preds(dl=[(x,y)]) preds . TensorImage([[2.1285e-01, 4.7995e-02, 1.1221e-03, ..., 1.4506e-03, 9.0126e-03, 2.4594e-02], [6.9040e-06, 4.3596e-03, 8.7159e-06, ..., 3.3018e-05, 2.8775e-05, 2.5054e-05], [2.1535e-12, 2.2470e-11, 5.4650e-12, ..., 2.8323e-10, 1.6942e-10, 4.7921e-11], ..., [1.0889e-07, 3.3818e-07, 1.8115e-05, ..., 4.5040e-07, 3.6186e-06, 5.4762e-08], [2.5651e-04, 5.6865e-05, 4.7429e-02, ..., 1.0788e-05, 2.0495e-04, 3.3897e-06], [1.6846e-05, 3.8460e-06, 1.8923e-06, ..., 1.2548e-06, 2.0216e-06, 2.9412e-07]]) . len(preds[0]),preds[0].sum() #37 pred and all add up to 1 . (37, TensorImage(1.0000)) . How did we manage to make all the pred add up to 1? Softmax! . Softmax . Below you will learn why softmax is better than sigmoid when dealing with multiple classes . plot_function(torch.sigmoid, min=-4,max=4) . /opt/conda/envs/fastai/lib/python3.8/site-packages/fastbook/__init__.py:73: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1603729096996/work/aten/src/ATen/native/RangeFactories.cpp:23.) x = torch.linspace(min,max) . acts = torch.randn((6,2))*2 #Getting preds acts . tensor([[ 0.6734, 0.2576], [ 0.4689, 0.4607], [-2.2457, -0.3727], [ 4.4164, -1.2760], [ 0.9233, 0.5347], [ 1.0698, 1.6187]]) . acts.sigmoid() #using sigmoid to squish pred (Notice that although the values are between 0-1, they dont add up to 1) . tensor([[0.6623, 0.5641], [0.6151, 0.6132], [0.0957, 0.4079], [0.9881, 0.2182], [0.7157, 0.6306], [0.7446, 0.8346]]) . Can fix by switching to softmax . sm_acts = torch.softmax(acts, dim=1) sm_acts #notice that now they add up to 1 . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . Sidebar: Unique indexing technique . targ = tensor([0,1,0,1,1,0]) . sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . idx = range(6) sm_acts[idx, targ] . tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661]) . from IPython.display import HTML df = pd.DataFrame(sm_acts, columns=[&quot;3&quot;,&quot;7&quot;]) df[&#39;targ&#39;] = targ df[&#39;idx&#39;] = idx df[&#39;loss&#39;] = sm_acts[range(6), targ] t = df.style.hide_index() #To have html code compatible with our script html = t._repr_html_().split(&#39;&lt;/style&gt;&#39;)[1] html = re.sub(r&#39;&lt;table id=&quot;([^&quot;]+)&quot; s*&gt;&#39;, r&#39;&lt;table &gt;&#39;, html) display(HTML(html)) . 3 7 targ idx loss . 0.602469 | 0.397531 | 0 | 0 | 0.602469 | . 0.502065 | 0.497935 | 1 | 1 | 0.497935 | . 0.133188 | 0.866811 | 0 | 2 | 0.133188 | . 0.996640 | 0.003360 | 1 | 3 | 0.003360 | . 0.595949 | 0.404051 | 1 | 4 | 0.404051 | . 0.366118 | 0.633882 | 0 | 5 | 0.366118 | . -sm_acts[idx, targ] . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . F.nll_loss(sm_acts, targ, reduction=&#39;none&#39;) . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . End Sidebar . Taking the Log . Another component of the cross entropy loss. . plot_function(torch.log, min=0,max=4) . loss_func = nn.CrossEntropyLoss() . loss_func(acts, targ) . tensor(1.8045) . F.cross_entropy(acts, targ) . tensor(1.8045) . nn.CrossEntropyLoss(reduction=&#39;none&#39;)(acts, targ) #Shows individual loses before taking the mean . tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048]) . Model Interpretation . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . interp.most_confused(min_val=5) . [(&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 9), (&#39;Egyptian_Mau&#39;, &#39;Bengal&#39;, 6), (&#39;beagle&#39;, &#39;basset_hound&#39;, 5), (&#39;english_setter&#39;, &#39;english_cocker_spaniel&#39;, 5), (&#39;staffordshire_bull_terrier&#39;, &#39;american_bulldog&#39;, 5), (&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 5)] . interp.plot_top_losses(5, nrows=1) . Improving Our Model . We can improve our model a variety of ways. For one, we can finetune the learning rate parameter. LR makes a very big difference in the model&#39;s performance. Below you will find a technique to find the optimal LR. . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(1, base_lr=0.1) #Current err is 40% - Very bad . epoch train_loss valid_loss error_rate time . 0 | 2.820359 | 3.461341 | 0.359269 | 01:07 | . epoch train_loss valid_loss error_rate time . 0 | 2.837471 | 1.268851 | 0.390392 | 01:24 | . The Learning Rate Finder . learn = cnn_learner(dls, resnet34, metrics=error_rate) lr_min,lr_steep = learn.--lr_find() #Finding best lr . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . Minimum/10: 1.00e-02, steepest point: 3.63e-03 . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2, base_lr=3e-3) #pick something inbetween . epoch train_loss valid_loss error_rate time . 0 | 1.285344 | 0.377785 | 0.117050 | 01:40 | . epoch train_loss valid_loss error_rate time . 0 | 0.522819 | 0.429013 | 0.128552 | 02:12 | . 1 | 0.318475 | 0.252019 | 0.075778 | 02:12 | . Error rate dropped down to 7% . Unfreezing . We can tune our model better by manually unfreezing it to find a better LR for the later layers. . fine_tune() . Below is what the finetune function does . learn.fine_tune?? . freeze() - Freezes the model first fit_one_cycle(1) - Runs 1 epoch to tune the final layers (Model frozen remember) base_lr /= 2 - Changes the lr unfreeze() - Now all parameters can be stepped self.fit_one_cycle - Now we fit the model on the number of given epochs (Input) . We will now implement fine_tune below . learn = cnn_learner(dls, resnet34, metrics=error_rate) #cnn_learner freezes model for us learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.134641 | 0.368042 | 0.112991 | 01:40 | . 1 | 0.526193 | 0.260705 | 0.083897 | 01:40 | . 2 | 0.330762 | 0.236595 | 0.075101 | 01:40 | . learn.unfreeze() . learn.lr_find() . SuggestedLRs(lr_min=7.585775847473997e-08, lr_steep=0.00013182566908653826) . learn.fit_one_cycle(3, lr_max=1e-5) #Update learning rate again and train some more . epoch train_loss valid_loss error_rate time . 0 | 0.276042 | 0.235768 | 0.077131 | 02:12 | . 1 | 0.237535 | 0.232880 | 0.071042 | 02:20 | . 2 | 0.216734 | 0.229787 | 0.071719 | 02:15 | . Didn&#39;t improve a lot but we can go even further! See below: . Discriminative Learning Rates . A technique of gradually increasing the LR for the later layers. . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(6, lr_max=slice(1e-6,1e-4)) #can do better using a slice . epoch train_loss valid_loss error_rate time . 0 | 1.136157 | 0.331316 | 0.108254 | 01:44 | . 1 | 0.526700 | 0.263616 | 0.078484 | 01:41 | . 2 | 0.318562 | 0.241009 | 0.075778 | 01:41 | . epoch train_loss valid_loss error_rate time . 0 | 0.239533 | 0.225810 | 0.073072 | 02:13 | . 1 | 0.240597 | 0.223987 | 0.069689 | 02:13 | . 2 | 0.203807 | 0.220246 | 0.064276 | 02:13 | . 3 | 0.202546 | 0.215731 | 0.066306 | 02:13 | . 4 | 0.188517 | 0.213268 | 0.066306 | 02:13 | . 5 | 0.177560 | 0.210809 | 0.065629 | 02:13 | . learn.recorder.plot_loss() . Freezing . We can even train our model on the final layers for more epochs, before training it on the entire architecture. . from fastai.callback.fp16 import * learn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16() #half as many bits (Half precision floating pts) learn.fine_tune(6, freeze_epochs=3) #First 3 epochs train finals layer, next 6 train all parameters . Downloading: &#34;https://download.pytorch.org/models/resnet50-19c8e357.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.426074 | 0.289685 | 0.094723 | 02:23 | . 1 | 0.630846 | 0.336782 | 0.109608 | 02:23 | . 2 | 0.421451 | 0.289481 | 0.083221 | 02:23 | . epoch train_loss valid_loss error_rate time . 0 | 0.265935 | 0.293698 | 0.094046 | 03:13 | . 1 | 0.291401 | 0.294177 | 0.081867 | 03:14 | . 2 | 0.256584 | 0.271347 | 0.077131 | 03:14 | . 3 | 0.164891 | 0.258964 | 0.073072 | 03:14 | . 4 | 0.088564 | 0.210342 | 0.064953 | 03:14 | . 5 | 0.051878 | 0.209877 | 0.063599 | 03:14 | . Conclusion . Overall I hope you learned how to create your own labels and finetune your model to perform better. . Questionnaire . Why do we first resize to a large size on the CPU, and then to a smaller size on the GPU? This proccess is actually known as presizing. Here a large image is needed because data augmentation often leads to degradation of the image. Therefore, to minize this destruction of the image quality, this technique known as presizing is used. | If you are not familiar with regular expressions, find a regular expression tutorial, and some problem sets, and complete them. Have a look on the book&#39;s website for suggestions. | What are the two ways in which data is most commonly provided, for most deep learning datasets? Individual files with data (Ex: Images) Tabular data (CSV data) | Look up the documentation for L and try using a few of the new methods that it adds. L is a custom list class by FastAI. It is designed to be a replacement for list in python. | Look up the documentation for the Python pathlib module and try using a few methods of the Path class. | Give two examples of ways that image transformations can degrade the quality of the data. Rotating an image by leaves corner regions of the new bounds with emptiness, which will not teach the model anything. Many rotation and zooming operations will require interpolation, which leave a lower quality image. | What method does fastai provide to view the data in a DataLoaders? DataLoader.show_batch() | What method does fastai provide to help you debug a DataBlock? DataBlock.summary() | Should you hold off on training a model until you have thoroughly cleaned your data? No. It is better to create a model first and then plot_top_losses to have the model help you clean the data. | What are the two pieces that are combined into cross-entropy loss in PyTorch? Softmax function and Negative Log Likelihood Loss | What are the two properties of activations that softmax ensures? Why is this important? All values add up to 1 and amplifies small changes in the output activations. This overall makes the model more confident when classifying. | When might you want your activations to not have these two properties? I guess when you have more than one label possible for a class. | Calculate the exp and softmax columns of &lt;&gt; yourself (i.e., in a spreadsheet, with a calculator, or in a notebook).&lt;/strong&gt; &lt;/li&gt; Why can&#39;t we use torch.where to create a loss function for datasets where our label can have more than two categories? torch.where can only select between two possibilities. | What is the value of log(-2)? Why? Undefined. Log is the inverse of exp, where all values are pos. | What are two good rules of thumb for picking a learning rate from the learning rate finder? Minimum/10 Subjective choice based on observation | What two steps does the fine_tune method do? Freezes and trains the head for 1 epoch Unfreezes and trains on the input epochs | In Jupyter Notebook, how do you get the source code for a method or function? ?? | What are discriminative learning rates? Trick of using different learning rates for different layers of the model. Here the early layers have a lower lr and the later layers have a higher lr. | How is a Python slice object interpreted when passed as a learning rate to fastai? First val is the initial lr, final val is the final lr, and the layers inbetween have a lr thats equal distant in the range. | Why is early stopping a poor choice when using 1cycle training? The training may not have time to reach lower learning rate values. | What is the difference between resnet50 and resnet101? Number of layers | What does to_fp16 do? Lowers the floating point precision numbers so you can speed up training. | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Further Research . Find the paper by Leslie Smith that introduced the learning rate finder, and read it. | See if you can improve the accuracy of the classifier in this chapter. What&#39;s the best accuracy you can achieve? Look on the forums and the book&#39;s website to see what other students have achieved with this dataset, and how they did it. Rather than doing this lesson, I decided to do the MNIST, for which I had an accuracy of 61%. After the LR improvement it jumped to 87%. | &lt;/div&gt; |",
            "url": "https://usama280.github.io/PasteBlogs/2021/07/10/Lesson5-pet_breeds.html",
            "relUrl": "/2021/07/10/Lesson5-pet_breeds.html",
            "date": " • Jul 10, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Lesson 4 P2 - FastAI",
            "content": "The MNIST Loss Function . In this lesson we will be recreating our ML model for the MNIST dataset. . Loading data from previous P1 . path = untar_data(URLs.MNIST_SAMPLE) #path for data Path.BASE_PATH = path threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() #getting 3&#39;s data from path sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() #getting 7&#39;s data from path seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 . Processing/reshaping data . It is neccessary we reshape our data so we can have all the images laid our in a single matrix . train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) #.view reshapes the image where each row has 1 image # with all its content in a single row (each image is 28x28) . train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) train_x.shape,train_y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . dset = list(zip(train_x,train_y)) #zip() creates a concatination of x,y x,y = dset[0] x.shape,y . (torch.Size([784]), tensor([1])) . valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . def init_params(size, var=1.0): return (torch.randn(size)*var).requires_grad_() . weights = init_params((28*28,1)) #weights needed for every pixel, hence 28*28 . bias = init_params(1) #Need bias because w*p = 0 when p=0 (p = pixel) . (train_x[0]*weights.T).sum() + bias #Must transpose so multi can happen . tensor([-6.2330], grad_fn=&lt;AddBackward0&gt;) . def linear1(xb): return xb@weights + bias #@ repersents matrix multi preds = linear1(train_x) preds #preds of all images . tensor([[ -6.2330], [-10.6388], [-20.8865], ..., [-15.9176], [ -1.6866], [-11.3568]], grad_fn=&lt;AddBackward0&gt;) . Sigmoid . Sigmoid is a function that is often used in ML to squish values between 0-1. As you may have noticed our predictions range very much. This can be an issue when handling the loss, so we make use of the sigmoid function to squish these values between 0-1. . def sigmoid(x): return 1/(1+torch.exp(-x)) . plot_function(torch.sigmoid, title=&#39;Sigmoid&#39;, min=-4, max=4) . /opt/conda/envs/fastai/lib/python3.8/site-packages/fastbook/__init__.py:73: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1603729096996/work/aten/src/ATen/native/RangeFactories.cpp:23.) x = torch.linspace(min,max) . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() #squishing predictions between 0-1 return torch.where(targets==1, 1-predictions, predictions).mean() . Sidebar: SGD and Mini-Batches . coll = range(15) dl = DataLoader(coll, batch_size=5, shuffle=True) #creates minibatches list(dl) . [tensor([ 3, 12, 8, 10, 2]), tensor([ 9, 4, 7, 14, 5]), tensor([ 1, 13, 0, 6, 11])] . ds = L(enumerate(string.ascii_lowercase)) ds . (#26) [(0, &#39;a&#39;),(1, &#39;b&#39;),(2, &#39;c&#39;),(3, &#39;d&#39;),(4, &#39;e&#39;),(5, &#39;f&#39;),(6, &#39;g&#39;),(7, &#39;h&#39;),(8, &#39;i&#39;),(9, &#39;j&#39;)...] . dl = DataLoader(ds, batch_size=6, shuffle=True) #Works with tuples as well list(dl) . [(tensor([17, 18, 10, 22, 8, 14]), (&#39;r&#39;, &#39;s&#39;, &#39;k&#39;, &#39;w&#39;, &#39;i&#39;, &#39;o&#39;)), (tensor([20, 15, 9, 13, 21, 12]), (&#39;u&#39;, &#39;p&#39;, &#39;j&#39;, &#39;n&#39;, &#39;v&#39;, &#39;m&#39;)), (tensor([ 7, 25, 6, 5, 11, 23]), (&#39;h&#39;, &#39;z&#39;, &#39;g&#39;, &#39;f&#39;, &#39;l&#39;, &#39;x&#39;)), (tensor([ 1, 3, 0, 24, 19, 16]), (&#39;b&#39;, &#39;d&#39;, &#39;a&#39;, &#39;y&#39;, &#39;t&#39;, &#39;q&#39;)), (tensor([2, 4]), (&#39;c&#39;, &#39;e&#39;))] . End Sidebar . Creating and testing with our own batch first . Before we go further, why don&#39;t we simulate what takes place with our own minibatch. . P0: Get batch . batch = train_x[:4] batch.shape . torch.Size([4, 784]) . linear1?? . P1: Initialize parameters . preds = linear1(batch) #Get pred (initialize weights) preds . tensor([[11.6180], [ 9.0489], [-2.4524], [-2.5197]], grad_fn=&lt;AddBackward0&gt;) . P2: Calc loss . loss = mnist_loss(preds, train_y[:4]) loss . tensor(0.4616, grad_fn=&lt;MeanBackward0&gt;) . mnist_loss?? . P3: Calc grad . loss.backward() weights.grad.shape, weights.grad.mean(), bias.grad . (torch.Size([784, 1]), tensor(-0.0057), tensor([-0.0355])) . Put the above into a single func . We can take everything we did above and make a simple function encompassing it . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . How to do an epoch . To do an epoch, we just need to grab each minibatch from the dataset, call calc_grad, and then step the weights. . def train_epoch(model, lr, params): for xb,yb in dl: #get x and y batch calc_grad(xb, yb, model) #Calc grad for p in params: p.data -= p.grad*lr #Update/take a step p.grad.zero_() #Set grad to zero . (preds&gt;0.0).float() == train_y[:4] . tensor([[ True], [ True], [False], [False]]) . How to calc accuracy . Accuracy is another function we need. Using the function below we can determine the performance of the model. Here, anytime the prediction is pos greater than .5 (Where pred was pos), it is correct. . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5).float() == yb #.5 because sigmoid(0) = .5 return correct.float().mean() . batch_accuracy(linear1(batch), train_y[:4]) . tensor(0.5000) . def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.5484 . Putting everything togather . Below we initialize the weights, create a dataloader, train, and test . weights = init_params((28*28,1)) bias = init_params(1) . dl = DataLoader(dset, batch_size=256) #create minibatches #We can grab the first batch and take a look at it xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . valid_dl = DataLoader(valid_dset, batch_size=256) #Create minibatch for validation set . lr = 1. params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.637 . for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1)) . 0.791 0.8925 0.9315 0.9476 0.9515 0.9579 0.9637 0.9652 0.9667 0.9672 0.9677 0.9701 0.9706 0.9711 0.9725 0.9735 0.9735 0.974 0.974 0.9745 . Congratulation you have created ur official ML model from scratch! . Let&#39;s now optimize what we did above . linear_model = nn.Linear(28*28,1) #Does exactly what out funtion linear1 does and initialzes our parameters for us . w,b = linear_model.parameters() w.shape,b.shape . (torch.Size([1, 784]), torch.Size([1])) . class BasicOptim: def __init__(self,params,lr): self.params = list(params) self.lr = lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . opt = BasicOptim(linear_model.parameters(), lr) . def train_epoch(model): for xb,yb in dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() . validate_epoch(linear_model) . 0.3794 . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . train_model(linear_model, 20) . 0.4932 0.7476 0.8511 0.9155 0.9346 0.9482 0.9555 0.9629 0.9658 0.9673 0.9707 0.9722 0.9736 0.9751 0.9761 0.9766 0.9775 0.9775 0.9785 0.9785 . Can further optimize by using FastAI ToolKit . linear_model = nn.Linear(28*28,1) #fastAI opt = SGD(linear_model.parameters(), lr) #fastAI train_model(linear_model, 20) . 0.4932 0.7393 0.8613 0.9175 0.9365 0.9497 0.957 0.9634 0.9663 0.9673 0.9702 0.9727 0.9736 0.9751 0.9761 0.9766 0.9775 0.978 0.978 0.979 . Finally, the most simplest way . dls = DataLoaders(dl, valid_dl) #NOT dataLoader, this class stores away the train and valid data into a single obj . learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(10, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.636365 | 0.503465 | 0.495584 | 00:00 | . 1 | 0.521433 | 0.170448 | 0.864082 | 00:00 | . 2 | 0.191138 | 0.189521 | 0.824828 | 00:00 | . 3 | 0.083866 | 0.109406 | 0.910697 | 00:00 | . 4 | 0.044337 | 0.079229 | 0.932777 | 00:00 | . 5 | 0.028900 | 0.063259 | 0.947007 | 00:00 | . 6 | 0.022547 | 0.053348 | 0.954367 | 00:00 | . 7 | 0.019723 | 0.046764 | 0.961727 | 00:00 | . 8 | 0.018294 | 0.042152 | 0.965653 | 00:00 | . 9 | 0.017439 | 0.038766 | 0.967615 | 00:00 | . I hope you now feel comfortable creating from scratch as well as using FastAI ToolKit where possible . Adding a Nonlinearity . We can improve our model by adding some nonlinearity to it. So far we have been using a simple linear classifier. A linear classifier is very constrained. To make it a perform better, we need to add something nonlinear between two linear classifiers—this is what gives us a neural network. . def simple_net(xb): res = xb@w1 + b1 #Linear func res = res.max(tensor(0.0)) #Activation func: ReLU res = res@w2 + b2 #Linear func return res . This is all you need to change to implement nonlinearity. Before we were using linear1. Compare this to that. . w1 = init_params((28*28,30)) b1 = init_params(30) w2 = init_params((30,1)) b2 = init_params(1) . plot_function(F.relu) . simple_net = nn.Sequential( nn.Linear(28*28,30), #30 sets of weights nn.ReLU(), nn.Linear(30,1) #convert back into 1 set of weights ) . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.324376 | 0.404988 | 0.505888 | 00:00 | . 1 | 0.150483 | 0.233413 | 0.803238 | 00:00 | . 2 | 0.083043 | 0.117188 | 0.912169 | 00:00 | . 3 | 0.054295 | 0.078788 | 0.940628 | 00:00 | . 4 | 0.040957 | 0.061365 | 0.956330 | 00:00 | . 5 | 0.034142 | 0.051610 | 0.962709 | 00:00 | . 6 | 0.030232 | 0.045451 | 0.965653 | 00:00 | . 7 | 0.027697 | 0.041252 | 0.967125 | 00:00 | . 8 | 0.025878 | 0.038205 | 0.968106 | 00:00 | . 9 | 0.024475 | 0.035891 | 0.970069 | 00:00 | . 10 | 0.023342 | 0.034062 | 0.972522 | 00:00 | . 11 | 0.022400 | 0.032572 | 0.973013 | 00:00 | . 12 | 0.021602 | 0.031321 | 0.973994 | 00:00 | . 13 | 0.020913 | 0.030250 | 0.973994 | 00:00 | . 14 | 0.020312 | 0.029316 | 0.974975 | 00:00 | . 15 | 0.019781 | 0.028492 | 0.976448 | 00:00 | . 16 | 0.019307 | 0.027757 | 0.977920 | 00:00 | . 17 | 0.018882 | 0.027094 | 0.978901 | 00:00 | . 18 | 0.018496 | 0.026495 | 0.978901 | 00:00 | . 19 | 0.018143 | 0.025950 | 0.979392 | 00:00 | . 20 | 0.017820 | 0.025452 | 0.979392 | 00:00 | . 21 | 0.017520 | 0.024996 | 0.979392 | 00:00 | . 22 | 0.017242 | 0.024576 | 0.979882 | 00:00 | . 23 | 0.016983 | 0.024189 | 0.980864 | 00:00 | . 24 | 0.016740 | 0.023832 | 0.981354 | 00:00 | . 25 | 0.016511 | 0.023501 | 0.981354 | 00:00 | . 26 | 0.016295 | 0.023195 | 0.981354 | 00:00 | . 27 | 0.016090 | 0.022910 | 0.981354 | 00:00 | . 28 | 0.015896 | 0.022645 | 0.982826 | 00:00 | . 29 | 0.015711 | 0.022398 | 0.982826 | 00:00 | . 30 | 0.015535 | 0.022167 | 0.982336 | 00:00 | . 31 | 0.015367 | 0.021952 | 0.982826 | 00:00 | . 32 | 0.015206 | 0.021750 | 0.982826 | 00:00 | . 33 | 0.015052 | 0.021560 | 0.982826 | 00:00 | . 34 | 0.014904 | 0.021382 | 0.982826 | 00:00 | . 35 | 0.014762 | 0.021215 | 0.982826 | 00:00 | . 36 | 0.014625 | 0.021057 | 0.982826 | 00:00 | . 37 | 0.014494 | 0.020908 | 0.982826 | 00:00 | . 38 | 0.014367 | 0.020767 | 0.982826 | 00:00 | . 39 | 0.014245 | 0.020634 | 0.982826 | 00:00 | . plt.plot(L(learn.recorder.values).itemgot(2)); . Sidebar: Now that we have trained our model, we can view its parameters . m = learn.model m . Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) . w, b = m[0].parameters() . w.shape . torch.Size([30, 784]) . w[0].view(28,28) . show_image(w[2].view(28,28)) . &lt;AxesSubplot:&gt; . Seems like this neuron was looking for curves . Going Deeper - Using ResNet . FastAI has various resnets anyone can use. Lets see how that compares to our learner. . dls = ImageDataLoaders.from_folder(path) learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1, 0.1) #Thats insane . epoch train_loss valid_loss accuracy time . 0 | 0.132423 | 0.036220 | 0.995093 | 00:17 | . Out preformed our model in a single epoch! Guess we still have a lot more to learn :) . Questionnaire . How is a grayscale image represented on a computer? How about a color image? Image on the computer are represented by a number value, where 0=white, 255=black, and the grayscale inbetween. A grayscale image is rank 2 (No color channels) A color image is rank 3 (Has the 3 color channels, RGB) | How are the files and folders in the MNIST_SAMPLE dataset structured? Why? Files are split into train, valid, labels. This makes it easier as the training and validation set have already been presplit for for. | Explain how the &quot;pixel similarity&quot; approach to classifying digits works. This is similer to the Nearest neighbors approach, where one compare each test image with all training images. Only here, the image being compared to is an average of all the training images. Then using a distance metric we can find the abs difference between the images to identify it. | What is a list comprehension? Create one now that selects odd numbers from a list and doubles them. A python condensing technique used with for-loop. | l = [i for i in range(20)] oddList = [i**2 for i in l if i%2 != 0] . What is a &quot;rank-3 tensor&quot;? A 3 dimensional tensor (Also known as a volumn). | What is the difference between tensor rank and shape? How do you get the rank from the shape? Rank refers to the number of dimensions in a tensor Shape is the size of each dimension of a tensor . Taking the len(shape) = rank . | What are RMSE and L1 norm? Loss functions | How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop? Broadcasting | Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers. t = tensor(list(range(1,10))).view(3,3) t[1:,0:2] . | What is broadcasting? A technique of applying an operation onto all values within an object, often, regardless of tensor (Exceptions do apply). | Are metrics generally calculated using the training set, or the validation set? Why? Validation set as it contains unseen data. | What is SGD? Optimization algorithm. This is what causes the loss to decrease as it steps/updates the parameters. | Why does SGD use mini-batches? Minibatches are faster and more efficient on GPU. Also, they gradient is calculated more appropriately as doing it across the entire batch could cause unstable and imprecise gradients. | What are the seven steps in SGD for machine learning? Initialize parameters Compute perdiction Get loss Get gradients Update wieghts Repeat Stop | How do we initialize the weights in a model? Randomly | What is &quot;loss&quot;? A metric used by the computer to determine its performance | Why can&#39;t we always use a high learning rate? Stepping to far can cause the model to increase loss or bounce and diverge | What is a &quot;gradient&quot;? Slope. This tell us how much we have to change each weight to make our model better. | Do you need to know how to calculate gradients yourself? No | Why can&#39;t we use accuracy as a loss function? A loss function needs to change as the weights are being adjusted. Accuracy only changes if the predictions of the model changes. | Draw the sigmoid function. What is special about its shape? Squishes all values between 0-1 | What is the difference between a loss function and a metric? The loss function is understood by the computer, while a metric is understood by us humans. | What is the function to calculate new weights using a learning rate? The optimizer step function (Ex: SGD). | What does the DataLoader class do? Creates minibatches | Write pseudocode showing the basic steps taken in each epoch for SGD. . for x,y in data: pred = model(x) loss = loss_func(pred, y) loss.backward() for p in self.params: p -= parameters.grad * lr p.grad = None . | Create a function that, if passed two arguments [1,2,3,4] and &#39;abcd&#39;, returns [(1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;c&#39;), (4, &#39;d&#39;)]. What is special about that output data structure? def func(l1,l2): return list(zip(l1,l2)) . | What does view do in PyTorch? Reshapes tensor | What are the &quot;bias&quot; parameters in a neural network? Why do we need them? So that the gradient isnt set to 0 during the first iteration. | What does the @ operator do in Python? Matrix multi | What does the backward method do? Calculated gradients | Why do we have to zero the gradients? PyTorch remembers the previously stored gradients | What information do we have to pass to Learner? dataset (DataLoaders), model (Ex: nn.Linear), opt func (Ex: SGD), loss func (Ex: mnist_loss), metric(Optional) | Show Python or pseudocode for the basic steps of a training loop. . def train_epoch(model,lr,params): for x,y in dl: calc_grad(x,y,model) for p in self.params: p -= parameters.grad * lr p.grad = None for i in range(epochs): train_epoch(model, lr, params) . | What is &quot;ReLU&quot;? Draw a plot of it for values from -2 to +2. Activation function . | What is an &quot;activation function&quot;? The purpose of an activation function is to add non-linearity to the model. | What&#39;s the difference between F.relu and nn.ReLU? F.relu is a Python function nn.ReLU is a PyTorch module (So part of a class) | The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more? There are performance benefits to using more than one nonlinearity | Further Research . Create your own implementation of Learner from scratch, based on the training loop shown in this chapter. | Complete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You&#39;ll need to do some of your own research to figure out how to overcome some obstacles you&#39;ll meet on the way. Completed, see here: https://usama280.github.io/PasteBlogs/ |",
            "url": "https://usama280.github.io/PasteBlogs/2021/07/07/Lesson4_P2-mnist_basics.html",
            "relUrl": "/2021/07/07/Lesson4_P2-mnist_basics.html",
            "date": " • Jul 7, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Lesson 4 P1 - FastAI",
            "content": "Learning the basics of classification . Now that we are comfortable creating models useing FastAI&#39;s toolkit, lets go back to the basics. Below is a simple MNIST dataset containing 3&#39;s and 7&#39;s. Lets try to classify this without using FastAI&#39;s toolkit. . Getting and viewing data . path = untar_data(URLs.MNIST_SAMPLE) #path for data . path.ls() . (#3) [Path(&#39;train&#39;),Path(&#39;labels.csv&#39;),Path(&#39;valid&#39;)] . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/3&#39;),Path(&#39;train/7&#39;)] . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() #getting 3&#39;s data from path sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() #getting 7&#39;s data from path threes . (#6131) [Path(&#39;train/3/10.png&#39;),Path(&#39;train/3/10000.png&#39;),Path(&#39;train/3/10011.png&#39;),Path(&#39;train/3/10031.png&#39;),Path(&#39;train/3/10034.png&#39;),Path(&#39;train/3/10042.png&#39;),Path(&#39;train/3/10052.png&#39;),Path(&#39;train/3/1007.png&#39;),Path(&#39;train/3/10074.png&#39;),Path(&#39;train/3/10091.png&#39;)...] . im3_path = threes[1] im3 = Image.open(im3_path) #shows image im3 . array(im3)[4:10,4:10] #numpy to convert image into quantitative rep . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . tensor(im3)[4:10,4:10] #same thing as numpy array but work better on GPU&#39;s (preferred) . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . im3.shape . (28, 28) . im3_t = tensor(im3) df = pd.DataFrame(im3_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) #Using panda&#39;s framework . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . First Try: Pixel Similarity . Why don&#39;t we begin by creating an ideal image of a three based on the training set. Then having that ideal image compared with the other images of 3&#39;s and 7&#39;s to classify them? . seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] len(three_tensors), len(seven_tensors) . (6131, 6265) . show_image(three_tensors[1]); . type(three_tensors) . list . Right now our tensors are lists. We must fix this by stacking them. . stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . len(stacked_threes.shape) #Returns rank. Rank means it has n dementions . 3 . stacked_threes.ndim . 3 . mean3 = stacked_threes.mean() #notice taking mean gives a number mean3 . tensor(0.1415) . mean3 = stacked_threes.mean(0) #the 0 represents the axis we are doing the mean across (In this case across the first axis 6131) show_image(mean3); . Here&#39;s our ideal 3 . mean7 = stacked_sevens.mean(0) show_image(mean7); . And our ideal 7, which we will also be using as a comparison . a_3 = stacked_threes[1] show_image(a_3); . Difference formula . To determine the difference between our ideal image and the training images we need to make use of some formula. Particularly, using either the mean absolute difference(L1 norm) or the root mean squared error(L2 norm). . dist_3_abs = (a_3 - mean3).abs().mean() #L1 norm dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt() #RMSE or L2 norm dist_3_abs,dist_3_sqr . (tensor(0.1114), tensor(0.2021)) . dist_7_abs = (a_3 - mean7).abs().mean() dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() dist_7_abs,dist_7_sqr . (tensor(0.1586), tensor(0.3021)) . F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt() . (tensor(0.1586), tensor(0.3021)) . Sidebar: NumPy Arrays and PyTorch Tensors . Just some comparison between tensors and numpy arrays . data = [[1,2,3],[4,5,6]] arr = array (data) tns = tensor(data) . arr # numpy . array([[1, 2, 3], [4, 5, 6]]) . tns # pytorch . tensor([[1, 2, 3], [4, 5, 6]]) . tns[1] . tensor([4, 5, 6]) . tns[:,1] . tensor([2, 5]) . tns[1,1:3] . tensor([5, 6]) . tns+1 . tensor([[2, 3, 4], [5, 6, 7]]) . tns.type() . &#39;torch.LongTensor&#39; . tns*1.5 . tensor([[1.5000, 3.0000, 4.5000], [6.0000, 7.5000, 9.0000]]) . End Sidebar . Training set . As before, lets create our training set, or validation set in this case. Similer to before, we will be stacking all the images. . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape, valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Loss function . This is our chosen distance formula. We will be using this to classify 3&#39;s and 7&#39;s. . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) #mean across 2nd and last axis (28,28) mnist_distance(a_3, mean3) . tensor(0.1114) . Computing Metrics Using Broadcasting . Broadcasting is a technique of vectorizing array operations so that looping occurs in C instead of Python. This is far more efficient and faster than looping. See below broadcasting in action, along with an example. . valid_3_dist = mnist_distance(valid_3_tens, mean3) #broadcast method across entire validation set valid_3_dist, valid_3_dist.shape . (tensor([0.1787, 0.1422, 0.1412, ..., 0.1358, 0.1301, 0.1110]), torch.Size([1010])) . tensor([1,2,3]) + tensor([1]) #broadcasting example . tensor([2, 3, 4]) . (valid_3_tens-mean3).shape . torch.Size([1010, 28, 28]) . def is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7) . a_7 = stacked_sevens[1] a_7 . show_image(a_7) . &lt;AxesSubplot:&gt; . is_3(a_7), is_3(a_7).float() #tensor val is 0 . (tensor(False), tensor(0.)) . Nice it&#39;s working! . is_3(valid_3_tens) #Broadcasting . tensor([True, True, True, ..., True, True, True]) . Testing . Now lets test it and view our accuracy! . accuracy_3s = is_3(valid_3_tens).float() .mean() accuracy_7s = (1 - is_3(valid_7_tens).float()).mean() accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . Wow 95% accuracy! You just created your very first model from scratch! . Lets test our model using our own image . uploader = widgets.FileUpload() uploader . img = PILImage.create(uploader.data[0]) img.to_thumb(40) . I drew this in paint . img = img.resize((28,28)) #Resizing img . t_3 = tensor(img) #converting to tensor . . t_3.shape . torch.Size([28, 28, 3]) . t_3 = t_3[:,:,0] #dropping channels . show_image(t_3) #Awesome now it looks like an image from our dataset . &lt;AxesSubplot:&gt; . t_3.shape . torch.Size([28, 28]) . is_3(t_3) #Nice, it got it right! . tensor(True) . Conclusion . I recommend you play around with this model we created, and try to spot some limitations to it. Although this is a ML model, it certainly is far from the ideal ML models. . Stochastic Gradient Descent (SGD) . What makes machine learning models truly learnable is SGD. Below you will see the influence of SGD on training a model. . An End-to-End SGD Example . time = torch.arange(0,20).float(); time . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) . speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1 plt.scatter(time,speed); . Lets assume we have this data given . def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . def mse(preds, targets): return ((preds-targets)**2).mean().sqrt() #mean squared error . Step 1: Initialize the parameters . All ML models have parameters. Parameters are random values that represent the influence of each value on the output. We often refer to these as weights. . params = torch.randn(3).requires_grad_() #Getting random weights and requiring gradient params . tensor([0.2815, 0.0562, 0.5227], requires_grad=True) . We are requiring gradients because this is what we will be using to improve our weights . Step 2: Calculate the predictions . Lets get a prediction using the random weights we initialzed above. . preds = f(time, params) . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) ax.set_ylim(-300,100) . show_preds(preds) #Red is our predictions, Blue is the labels . Not bad for an initial prediction . Step 3: Calculate the loss . Lets now calculate the loss. Loss is a metric that is used by the model to determine it&#39;s performance. . loss = mse(preds, speed) #Current loss loss . tensor(35.6327, grad_fn=&lt;SqrtBackward&gt;) . Step 4: Calculate the gradients . To improve the loss, the gradients of the weights need to be calculated. Why the gradient? Because the gradient gives us the slope, which we need to determine how to step the weights. . loss.backward() params.grad . tensor([121.4830, 7.8875, 0.3013]) . params.grad * 1e-5 #1e-5 is the learning rate . tensor([1.2148e-03, 7.8875e-05, 3.0131e-06]) . Step 5: Step the weights. . Now that we have everything, lets update our weights . lr = 1e-4 params.data -= lr * params.grad.data params.grad = None . preds = f(time,params) mse(preds, speed) #loss improved . tensor(34.1798, grad_fn=&lt;SqrtBackward&gt;) . Notice that our loss has improved with these new weights . show_preds(preds) . Putting all of the above into a simple function . Lets grab all the steps we did above and put it into one function for simplicity. . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . Step 6: Repeat the process (Training) . Now we just need to trian our model. . for i in range(10): apply_step(params) #running it 10 times (Notice loss improving) . 34.17982482910156 32.844730377197266 31.63145637512207 30.54193687438965 29.575712203979492 28.729764938354492 27.998600006103516 27.374542236328125 26.84825325012207 26.409330368041992 . _,axs = plt.subplots(1,4,figsize=(12,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . Conclusion . Compare this approach to the pixel similerity example. You will notice that here it truly seems like the model is learning: This is the approach we will be using from now on as it better fits the ideal ML model. . I have chosen to split this lesson into 2 parts. On the next lesson we will apply what we have learned to the MNIST dataset once again. This time using the more approriate ML approach. .",
            "url": "https://usama280.github.io/PasteBlogs/2021/07/07/Lesson4_P1-mnist_basics.html",
            "relUrl": "/2021/07/07/Lesson4_P1-mnist_basics.html",
            "date": " • Jul 7, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Lesson 2 - FastAI",
            "content": "Creating our own model . Lets create our very own bear classifier using images we grab from online! . Getting data from online . from pathlib import Path root = Path().cwd()/&quot;images&quot; #rmtree(root) #Deletes all previous images from jmd_imagescraper.core import * duckduckgo_search(root, &quot;Grizzly&quot;, &quot;Grizzly bears&quot;, max_results=100) duckduckgo_search(root, &quot;Black&quot;, &quot;Black bears&quot;, max_results=100) duckduckgo_search(root, &quot;Teddy&quot;, &quot;Teddy bears&quot;, max_results=100) . from jmd_imagescraper.imagecleaner import * display_image_cleaner(root) . From Data to DataBlock . Now that we have grabed our data, we need to make is usable for the model. To do this we make use of a DataBlock! DataBlock is a FastAI tool that makes it very easy to process data! . bears = DataBlock( blocks=(ImageBlock, CategoryBlock), #Assign independent and dependent blocks get_items=get_image_files, #Send the images splitter=RandomSplitter(valid_pct=0.2, seed=42), #Split validation data get_y=parent_label, #Get labels item_tfms=Resize(128)) #Resize all images to be same . dls = bears.dataloaders(root) #path goes here (I named it root) . Viewing our data . dls.valid.show_batch(max_n=4, nrows=1) . Augmentation on data . We can, and often do, augment our dataset. This makes the model more robust and can improve the models performance. Below are various augmentation techniques. . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = bears.dataloaders(root) dls.valid.show_batch(max_n=4, nrows=1) . Squishing . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = bears.dataloaders(root) dls.valid.show_batch(max_n=4, nrows=1) . Padding . bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = bears.dataloaders(root) dls.train.show_batch(max_n=4, nrows=1, unique=True) . Cropping . bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = bears.dataloaders(root) dls.train.show_batch(max_n=8, nrows=2, unique=True) . bears = bears.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = bears.dataloaders(root) . Training the Model . Now that we have our data, lets train it! To do so, we just need to create a learner: Simply give the following inputs:The dataloader, the model(resnet#), and metric . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.610175 | 0.412844 | 0.183333 | 00:03 | . epoch train_loss valid_loss error_rate time . 0 | 0.618662 | 0.107484 | 0.050000 | 00:04 | . 1 | 0.442147 | 0.103292 | 0.033333 | 00:04 | . 2 | 0.352873 | 0.157721 | 0.050000 | 00:04 | . 3 | 0.311809 | 0.171524 | 0.050000 | 00:04 | . Seems like our model did pretty good! . Cleaning our data . Now that we have trained our model, we can see what it got wrong. This can be very helpful in cleaning our dataset for bad images. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(5, nrows=1) . Notice that the last image isn&#39;t even a bear -- We should remove that! . cleaner = ImageClassifierCleaner(learn) cleaner . Exporting Your Model . You may be wondering how we can use our model elsewhere? Well to do that, simply export your model! . learn.export() . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . Now lets load that model and test it! . learn_inf = load_learner(path/&#39;export.pkl&#39;) . uploader = widgets.FileUpload() uploader . img = PILImage.create(uploader.data[0]) img.to_thumb(192) . learn_inf.dls.vocab . [&#39;Black&#39;, &#39;Grizzly&#39;, &#39;Teddy&#39;] . learn_inf.predict(img) . (&#39;Teddy&#39;, TensorImage(2), TensorImage([8.5309e-07, 2.1513e-07, 1.0000e+00])) . It&#39;s correct, nice! . Lets try a completely different image for the fun of it :) . uploader = widgets.FileUpload() uploader . img = PILImage.create(uploader.data[0]) img.to_thumb(192) . learn_inf.predict(img) . (&#39;Teddy&#39;, TensorImage(2), TensorImage([0.4078, 0.1477, 0.4445])) . Haha its between black bear and teddy bear - thats intresting . Creating a Notebook App from the Model . We can also create a notebook type app using widgets! See below. . btn_upload = widgets.FileUpload() btn_upload . img = PILImage.create(btn_upload.data[-1]) . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . pred,pred_idx,probs = learn_inf.predict(img) . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . VBox([widgets.Label(&#39;Select your bear!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . Voila . . #%watermark -v -m -p pandas,numpy,watermark,fastbook,voila,fastai . Conclusion . I hope you can now create and train your own models using the FastAI toolkit! . Questionnaire . Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data. Since most of the images are of bears caught in good light and close to the camera, it would work poorly if subjected to blurry, far away, poor lighting photos of bears. | Where do text models currently have a major deficiency? They can generate context-appropriate text, but struggle generating correct responses. For example, taking knowledge of medical information to generate medically correct responses. | What are possible negative societal implications of text generation models? Because it can generate compelling responses based on the context, it could cause people to assume it is correct, when infact it is not. So, simply put, distribution of misinformation on a potentially large scale. | In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? Having a human overseer. | What kind of tabular data is deep learning particularly good at? High cardinality (a lot of unique value) categorical columns, such as zip code or product ID. | What&#39;s a key downside of directly using a deep learning model for recommendation systems? The recommendation system is actually not very helpful and bias. For example, if you like the book Legend by Marie Lu (great book btw), it will suggest other books by the same author like Prodigy and Champion. Well, these are part of the same series so this is actually not very helpful. | What are the steps of the Drivetrain Approach? Define objective, Levers (Hyperparameters), Data collection, and Models | How do the steps of the Drivetrain Approach map to a recommendation system? Objective is to drive the sales. Levers is the ranking of the recommendations. Data is collected to generate recommendations that will cause new sales. | Create an image recognition model using data you curate, and deploy it on the web. | What is DataLoaders? A fastai class that stores multiple DataLoader objects you pass to it, and makes them available as train and valid. | What four things do we need to tell fastai to create DataLoaders? Type of data How to get data How to get Labels How to create validation set | What does the splitter parameter to DataBlock do? Splits data into subsets of train and validation set. | How do we ensure a random split always gives the same validation set? Define a seed. | What letters are often used to signify the independent and dependent variables? x is the independent, while y is the dependent. | What&#39;s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others? Crop cuts part of the image to the assigned size. Pad adds 0 pixels to the sides of the image. Squish either squishes or stretches the image.&lt;/br&gt; Choosing the right one depends on the data and problem. | What is data augmentation? Why is it needed? Data augmentation creates variation of the input data. This makes the model more robust (generalizable) and creates a larger dataset if the dataset is small. | What is the difference between item_tfms and batch_tfms? item_tfms are transformations applied to a single data sample. batch_tfms are applied to batched data samples. | What is a confusion matrix? A representation of the predictions vs the labels. Looks like an identity matrix. | What does export save? The model we trained. | What is it called when we use a model for getting predictions, instead of training? Inference | What are IPython widgets? IPython widgets is a mix of JavaScript and Python functionalities that let us build and interact with GUI components within Jupyter notebooks. For example, the uploader button seen above. | When might you want to use CPU for deployment? When might GPU be better? CPU is good/cost effective when analyzing single pieces of data at a time. GPUs are best for doing work in parallel or in batches at a time. | What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? Network connection could cause latency problems. | What are three examples of problems that could occur when rolling out a bear warning system in practice? Blurry/low resolution images Bears at night (Night images) Bears are obstructed by trees | What is &quot;out-of-domain data&quot;? Data that was not present within models training input. | What is &quot;domain shift&quot;? Type of data changes gradually over time. | What are the three steps in the deployment process? https://raw.githubusercontent.com/fastai/fastbook/780b76bef3127ce5b64f8230fce60e915a7e0735/images/att_00061.png | Further Research . Consider how the Drivetrain Approach maps to a project or problem you&#39;re interested in. | When might it be best to avoid certain types of data augmentation? For example, if the features in the images take up the whole image, then cropping may result in loss of information: Here, squishing or padding may be more useful. | For a project you&#39;re interested in applying deep learning to, consider the thought experiment &quot;What would happen if it went really, really well?&quot; | Start a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you&#39;re interested in. https://usama280.github.io/PasteBlogs/ |",
            "url": "https://usama280.github.io/PasteBlogs/2021/07/01/Lesson2-production.html",
            "relUrl": "/2021/07/01/Lesson2-production.html",
            "date": " • Jul 1, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Lesson 1 - FastAI",
            "content": "New to ML? Don&#39;t know where to start? . Machine learning may seem complex at first, given the math, background understanding, and code involved. However, if you truly want to learn, the best place to start is by building and messing around with a model. FastiAI makes it super easy to create and modify models to best solve your problem! Don&#39;t worry too much if you don&#39;t understand, we will get there. . Our First Model . As I have said above, the best way to learn is by actually creating your first model . from fastai.vision.all import * #IMPORT path = untar_data(URLs.PETS)/&#39;images&#39; #DATA SET def is_cat(x): return x[0].isupper() #Labels for the dataset (This dataset cat labels begin w/ uppercase letter) #Create dataset (Training data, test data) and correctly gets imgs w/ labels dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) #Creating architecture learn.fine_tune(1) #Training . epoch train_loss valid_loss error_rate time . 0 | 0.154482 | 0.022808 | 0.007442 | 00:56 | . epoch train_loss valid_loss error_rate time . 0 | 0.057639 | 0.017561 | 0.006089 | 01:09 | . Look at that, we created our first model and all with a few lines of code. . Why don&#39;t we test our cat classifier? . img = PILImage.create(image_cat()) img.to_thumb(192) . uploader = widgets.FileUpload() uploader . img = PILImage.create(uploader.data[0]) img.to_thumb(192) . is_cat,_,probs = learn.predict(img) print(f&quot;Is this a cat?: {is_cat}.&quot;) print(f&quot;Probability it&#39;s a cat: {probs[1].item():.6f}&quot;) . Is this a cat?: True. Probability it&#39;s a cat: 0.999998 . Fantastic, you can now classify cats! . Deep Learning Is Not Just for Image Classification . Often people think machine learning model are used only for images, this is not true at all! Below you will see numerous other types of models, each with its benifits! . A segmentation model . path = untar_data(URLs.CAMVID_TINY) dls = SegmentationDataLoaders.from_label_func( #Segmentation path, bs=8, fnames = get_image_files(path/&quot;images&quot;), label_func = lambda o: path/&#39;labels&#39;/f&#39;{o.stem}_P{o.suffix}&#39;, codes = np.loadtxt(path/&#39;codes.txt&#39;, dtype=str) ) learn = unet_learner(dls, resnet34) learn.fine_tune(8) . epoch train_loss valid_loss time . 0 | 2.821857 | 3.201599 | 00:05 | . epoch train_loss valid_loss time . 0 | 2.189840 | 1.802189 | 00:04 | . 1 | 1.831589 | 1.741264 | 00:04 | . 2 | 1.633543 | 1.287880 | 00:04 | . 3 | 1.459935 | 1.189295 | 00:04 | . 4 | 1.315327 | 1.003694 | 00:04 | . 5 | 1.188064 | 0.926959 | 00:04 | . 6 | 1.080196 | 0.872466 | 00:04 | . 7 | 0.997371 | 0.871319 | 00:04 | . learn.show_results(max_n=2, figsize=(10,12)) . A natural language model . from fastai.text.all import * dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;) learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) learn.fine_tune(2, 1e-2) . learn.predict(&quot;I really liked that movie!&quot;) . (&#39;pos&#39;, tensor(1), tensor([0.0228, 0.9772])) . A salary prediction model (Regression) . from fastai.tabular.all import * path = untar_data(URLs.ADULT_SAMPLE) dls = TabularDataLoaders.from_csv(path/&#39;adult.csv&#39;, path=path, y_names=&quot;salary&quot;, cat_names = [&#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;], cont_names = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education-num&#39;], procs = [Categorify, FillMissing, Normalize]) learn = tabular_learner(dls, metrics=accuracy) . learn.fit_one_cycle(3) . epoch train_loss valid_loss accuracy time . 0 | 0.372949 | 0.361306 | 0.833077 | 00:05 | . 1 | 0.354939 | 0.348455 | 0.841830 | 00:05 | . 2 | 0.349614 | 0.347378 | 0.840756 | 00:05 | . The below is a reccomendation model (AKA Regression model) . from fastai.collab import * path = untar_data(URLs.ML_SAMPLE) dls = CollabDataLoaders.from_csv(path/&#39;ratings.csv&#39;) learn = collab_learner(dls, y_range=(0.5,5.5)) learn.fine_tune(10) . epoch train_loss valid_loss time . 0 | 1.523757 | 1.424118 | 00:00 | . epoch train_loss valid_loss time . 0 | 1.376706 | 1.363828 | 00:00 | . 1 | 1.282471 | 1.173324 | 00:00 | . 2 | 1.034186 | 0.848724 | 00:00 | . 3 | 0.805283 | 0.694374 | 00:00 | . 4 | 0.709625 | 0.654900 | 00:00 | . 5 | 0.652975 | 0.645875 | 00:00 | . 6 | 0.634861 | 0.639299 | 00:00 | . 7 | 0.611313 | 0.637229 | 00:00 | . 8 | 0.617857 | 0.636715 | 00:00 | . 9 | 0.612095 | 0.636567 | 00:00 | . learn.show_results() . userId movieId rating rating_pred . 0 87.0 | 48.0 | 5.0 | 4.045295 | . 1 73.0 | 92.0 | 4.0 | 4.072179 | . 2 66.0 | 26.0 | 3.0 | 4.015839 | . 3 66.0 | 30.0 | 3.0 | 3.367572 | . 4 4.0 | 46.0 | 3.5 | 3.269851 | . 5 82.0 | 84.0 | 4.0 | 3.817361 | . 6 90.0 | 79.0 | 4.0 | 4.012848 | . 7 61.0 | 65.0 | 4.0 | 3.507185 | . 8 88.0 | 7.0 | 4.5 | 4.166433 | . Conclusion . I hope you feel more comfortable with machine learning and recognize the many benefits it can serve you :) . Questionnaire . Do you need these for deep learning? . Lots of math T / F | Lots of data T / F | Lots of expensive computers T / F | A PhD T / F | . | Name five areas where deep learning is now the best in the world. Vision, Natural language processing, Medicine, Robotics, and Games . | What was the name of the first device that was based on the principle of the artificial neuron? Mark I Perceptron | Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)? Processing units, State of activation, Output function, Pattern of connectivity, Propagation rule, Activation rule, Learning rule, Environment | What were the two theoretical misunderstandings that held back the field of neural networks? Single layer network unable to learn simple mathimatical functions. More layers make network too big and slow to be useful. | What is a GPU? A graphics card is a processor that can handle 1000&#39;s of tasks at the same time. Particularly great for deep learning. | Open a notebook and execute a cell containing: 1+1. What happens? 2 | Follow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen. | Complete the Jupyter Notebook online appendix. | Why is it hard to use a traditional computer program to recognize images in a photo? They are missing the weight assignment needed to recognize patterns within images to accomplish the task. | What did Samuel mean by &quot;weight assignment&quot;? The weight is another form of input that has direct influence on the model&#39;s performance. | What term do we normally use in deep learning for what Samuel called &quot;weights&quot;? Parameters | Draw a picture that summarizes Samuel&#39;s view of a machine learning model. https://vikramriyer.github.io/assets/images/machine_learning/fastai/model.jpeg | Why is it hard to understand why a deep learning model makes a particular prediction? There are many layers, each with numerous neurons. Therefore, it gets complex really fast what each neuron is looking for when viewing an image, and how that impacts the perediction. | What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy? Universal approximation theorem | What do you need in order to train a model? Data with labels | How could a feedback loop impact the rollout of a predictive policing model? The more the model is used the more biased the data becomes, and therefore, the more bias the model becomes. | Do we always have to use 224×224-pixel images with the cat recognition model? No. | What is the difference between classification and regression? Classification is about categorizing/labeling objects. Regression is about predicting numerical quantities, such as temp. | What is a validation set? What is a test set? Why do we need them? The validation set measures the accuracy of the model during training. The test set is used during the final evaluation to test the accuracy of the model. . We need both of them because the validation set could cause some bias in the model as we would are fitting the model towards it during training. However, the test set removes this and evaluates the model on unseen data, thereby, giving an accurate metric of accuracy. . | What will fastai do if you don&#39;t provide a validation set? Fastai will automatically create a validation dataset for us. | Can we always use a random sample for a validation set? Why or why not? It is not reccomended where order is neccessary, example ordered by time. | What is overfitting? Provide an example. This is when the model begins to fit to the training data rather than generalizing for similar unseen datasets. For example a model that does amazing on the training data, but performs poorly on test data: Good indication that model may have overfitted. | What is a metric? How does it differ from &quot;loss&quot;? The loss is the value calculated by the model to determine the impact each neuron has on the end result: Therefore, the value is used by models to measure its performance. The metric gives us, humans, an overall value of how accurate the model was: Therefore, a value we use to understand the models performance. | How can pretrained models help? A pretrained model already has the fundementals. Therefore, it can use this prior knowledge to learn faster and perform better on similer datasets. | What is the &quot;head&quot; of a model? The final layers from the pretrained model that have been replaced with new layers (w/ randomized weights) to better align with our dataset. These final layers are often the only thing trained while the rest of the model is frozen. | What kinds of features do the early layers of a CNN find? How about the later layers? The early layers often extract simple features like edges. The later layers are more complex and can identify advanced features like faces. | Are image models only useful for photos? No. Lots of other forms of data can be converted into images that can be used to solve such non-photo data problems. | What is an &quot;architecture&quot;? This is the structure of the model we use to solve the problem. | What is segmentation? Method of labeling all pixels within an image and masking it. | What is y_range used for? When do we need it? Specifies the range of values that can be perdicted by model. For example, movie rating&#39;s 0-5. | What are &quot;hyperparameters&quot;? These are the parameters that we can adjust to help the model perform better (Ex: Epochs). | What&#39;s the best way to avoid failures when using AI in an organization? Begin with the most simplest model and then slowly building up to more complexity. This way you have something working and don&#39;t get lost as you add onto the model. | Further Research . Each chapter also has a &quot;Further Research&quot; section that poses questions that aren&#39;t fully answered in the text, or gives more advanced assignments. Answers to these questions aren&#39;t on the book&#39;s website; you&#39;ll need to do your own research! . Why is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning? &lt;/br&gt; Modern GPUs provide a far superior processing power, memory bandwidth, and efficiency over the CPU. . | Try to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice. &lt;/br&gt; I believe feedback loops are primarly great for recommendation models. This is because the feedback loops create a bias model. For example, if a viewer like a movie, he/she will like similer movies. Being bias here towards particular types of movie is the best way to keep the viewer engaged. . |",
            "url": "https://usama280.github.io/PasteBlogs/2021/06/28/Lesson1-intro.html",
            "relUrl": "/2021/06/28/Lesson1-intro.html",
            "date": " • Jun 28, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://usama280.github.io/PasteBlogs/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://usama280.github.io/PasteBlogs/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Usama! . PasteBlog is something I started when I first started my training in Machine Learning at Illinois State University. I am very ambitious and explorative within the grand field of Computer Science: I enjoy programming, game development, web design, and, of course, machine learning! There is no ‘one thing’ I do as I have an ever-expanding range of interests and goals. Hope you enjoy the blogs and please don’t hesitate to reach out to me for any suggestions! . My LinkedIn: https://www.linkedin.com/in/usama-nad3em/ My Git: https://github.com/usama280 .",
          "url": "https://usama280.github.io/PasteBlogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://usama280.github.io/PasteBlogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}